{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Plow that show the distribution of the number of items per session\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ecommerce= pd.read_json('1_ecommerce.jsonl', lines=True)\n",
    "df_ecommerce.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each session, we will create a list of items that the user has clicked on, removing duplicates\n",
    "events_list = []\n",
    "for i in df_ecommerce.events:\n",
    "    clicks = []\n",
    "    for j in i:\n",
    "        if j['type'] == 'clicks':\n",
    "            clicks.append(j['aid'])\n",
    "    # remove duplicates from clicks\n",
    "    # clicks = set(clicks)\n",
    "    events_list.append(list(clicks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ecommerce['items'] = events_list\n",
    "df = df_ecommerce.drop(columns=[\"events\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes only the session with items length higher than 20\n",
    "df_truncated = df[df['items'].apply(lambda x: len(x) > 20)]\n",
    "df_truncated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df_truncated.copy()\n",
    "# redefine index    \n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode the 'items' column to create one row per item per session\n",
    "df_exploded_items = df.explode('items')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exploded_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count how many times each item appears in the dataset\n",
    "item_counts = df_exploded_items['items'].value_counts()\n",
    "item_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a plot of the distribution of the number of each item in the dataset\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(item_counts, bins=500, color='blue', alpha=0.7)\n",
    "plt.title('Distribution of the Number of Items per Session')\n",
    "plt.xlabel('Number of Items')\n",
    "plt.ylabel('Frequency')\n",
    "# set x-axis limits to 0-100\n",
    "plt.xlim(0, 100)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all the items that appear less than 10 times in the dataset\n",
    "item_counts = item_counts[item_counts > 10]\n",
    "\n",
    "# re,pve each row in the dataset that contains an item that appears less than 10 times in the dataset\n",
    "df_exploded_items = df_exploded_items[df_exploded_items['items'].isin(item_counts.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each item in each session and drop duplicates\n",
    "df_exploded_items['item_count'] = df_exploded_items.groupby(['session', 'items'])['items'].transform('count')\n",
    "df_exploded_items = df_exploded_items.drop_duplicates(subset=['session', 'items'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cerca un valore che ha item_count > 1\n",
    "df_exploded_items[df_exploded_items['item_count'] > 1].sort_values(by='item_count', ascending=False).head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "# Set the title of the plot\n",
    "plt.title('Distribution of the number of items per session')\n",
    "# Set the x and y labels\n",
    "plt.xlabel('Number of items')\n",
    "plt.ylabel('Frequency')\n",
    "# Create the histogram\n",
    "plt.hist(df_exploded_items['item_count'], bins=50, color='blue', alpha=0.7)\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# work only with the first 1000 sessions of the dataset\n",
    "#df_exploded_items = df_exploded_items.head(100_000)\n",
    "#df_exploded_items = df_exploded_items.reset_index(drop=True)\n",
    "df_exploded_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the itemCount in a float number\n",
    "#df_exploded_items['item_count'] = df_exploded_items['item_count'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the index of the dataframe as session id\n",
    "#df_exploded_items['session'] = df_exploded_items.index\n",
    "df_exploded_items['items'] = df_exploded_items['items'].astype('category')\n",
    "df_exploded_items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot = df_exploded_items.pivot_table(index='session', columns='items', values='item_count', aggfunc='first')\n",
    "df_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Supponiamo che df_pivot sia già stato creato come segue:\n",
    "# df_pivot = df_exploded_items.pivot_table(index='session', columns='items', values='item_count', aggfunc='sum')\n",
    "# Per questa implementazione, consideriamo il campo item_count come rating\n",
    "\n",
    "# Convertiamo il DataFrame in una matrice numpy, mantenendo i NaN per le celle mancanti\n",
    "R_df = df_pivot.copy()\n",
    "# Creiamo una maschera: 1 per gli elementi osservati, 0 altrimenti\n",
    "mask = (~R_df.isna()).astype(float).values\n",
    "\n",
    "# Sostituiamo i NaN in R con 0 (non verranno usati nel calcolo dell'errore grazie alla maschera)\n",
    "R = R_df.fillna(0).values\n",
    "\n",
    "def unconstrained_matrix_factorization(R, mask, k=10, epochs=100, alpha=0.001):\n",
    "    \"\"\"\n",
    "    R: matrice dei rating (numpy array) ottenuta dal pivot, con 0 per le celle mancanti\n",
    "    mask: matrice con 1 per gli elementi osservati e 0 per i mancanti\n",
    "    k: numero di fattori latenti\n",
    "    epochs: numero di iterazioni\n",
    "    alpha: learning rate\n",
    "    \"\"\"\n",
    "    num_users, num_items = R.shape\n",
    "    # Inizializzazione casuale delle matrici dei fattori\n",
    "    U = np.random.rand(num_users, k)\n",
    "    V = np.random.rand(num_items, k)\n",
    "    \n",
    "    losses = []  # per tenere traccia della funzione di costo\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Calcolo delle predizioni\n",
    "        R_hat = U.dot(V.T)\n",
    "        # Calcolo dell'errore solo sugli elementi osservati\n",
    "        error = (R - R_hat) * mask\n",
    "        # Calcolo della funzione di costo (loss)\n",
    "        cost = np.sum(error**2) / 2\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, costo: {cost:.4f}\")\n",
    "        # Aggiornamento dei fattori tramite gradient descent\n",
    "\n",
    "        # Calcolo del gradiente\n",
    "        U_grad = error.dot(V)\n",
    "        V_grad = error.T.dot(U)\n",
    "        \n",
    "        U += alpha * U_grad\n",
    "        V += alpha * V_grad\n",
    "\n",
    "        losses.append(cost)\n",
    "    \n",
    "    return U, V\n",
    "\n",
    "# Eseguiamo la fattorizzazione\n",
    "k = 10         # ad esempio, 10 fattori latenti\n",
    "epochs = 100   # numero di iterazioni\n",
    "alpha = 0.001  # learning rate\n",
    "\n",
    "U, V = unconstrained_matrix_factorization(R, mask, k=k, epochs=epochs, alpha=alpha)\n",
    "\n",
    "# Una volta appresi U e V, la matrice dei rating completa è data da:\n",
    "R_complete = U.dot(V.T)\n",
    "\n",
    "# Ad esempio, per predire il rating (item_count) della sessione 0 sull'item con indice 5:\n",
    "predicted_rating = R_complete[0, 5]\n",
    "print(f\"Predicted rating per sessione 0, item 5: {predicted_rating:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nerfstudio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
