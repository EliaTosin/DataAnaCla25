{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ],
   "id": "78569ddbc861490d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Load all data",
   "id": "31b6ad949d6cc38b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def load_news(news_path):\n",
    "    df = pd.read_csv(news_path, sep='\\t', header=None,\n",
    "                     names=['news_id', 'category', 'subcategory', 'title', 'abstract', 'url', 'title_entities', 'abstract_entities'])\n",
    "    df['content'] = df['title'].fillna('') + ' ' + df['abstract'].fillna('')\n",
    "    return df[['news_id', 'content']]\n",
    "\n",
    "train_news = load_news('MINDsmall_train/news.tsv')\n",
    "dev_news = load_news('MINDsmall_dev/news.tsv')\n",
    "# Check for duplicates on news_id and content\n",
    "all_news = pd.concat([train_news, dev_news]).drop_duplicates(['news_id', 'content'])"
   ],
   "id": "4152fed124a35dc4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "all_news.head()",
   "id": "74f58eccd74cb7df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "all_news.loc[0].content",
   "id": "520c7aacf7f5adbc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "all_news.describe()",
   "id": "e4b00937748d6c6c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Size of the train dataset:\", train_news.shape[0])\n",
    "print(\"Size of the test dataset:\", dev_news.shape[0])"
   ],
   "id": "b847f4d7d166f68"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "train TF-IDF",
   "id": "8a56254e906a83cd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "news_vectors = vectorizer.fit_transform(train_news['content'])  # only train content for training\n",
    "\n",
    "# Map news_id to index\n",
    "news_id_to_index = dict(zip(train_news['news_id'], range(len(train_news))))"
   ],
   "id": "f5aed9b66b2fdad0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Load Training Behaviors and Build User Profiles",
   "id": "dd7b9c1857922f23"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_behaviors = pd.read_csv('MINDsmall_train/behaviors.tsv', sep='\\t', header=None,\n",
    "                              names=['impression_id', 'user_id', 'time', 'history', 'impressions'])\n",
    "\n",
    "user_profiles = {}\n",
    "\n",
    "for _, row in train_behaviors.iterrows():\n",
    "    user = row['user_id']\n",
    "    history = row['history']\n",
    "    if pd.isna(history): continue\n",
    "    history_ids = [nid for nid in history.split() if nid in news_id_to_index]\n",
    "    if not history_ids: continue\n",
    "    indices = [news_id_to_index[nid] for nid in history_ids]\n",
    "    profile_vector = news_vectors[indices].mean(axis=0)\n",
    "    user_profiles[user] = profile_vector"
   ],
   "id": "adb75d7aeda3628"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Calculate embeddings",
   "id": "e52a0bcc20b0514b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load a pre-trained SentenceTransformer model for generating sentence embeddings\n",
    "# 'all-MiniLM-L6-v2' (384-dimensional vector space) is a lightweight, efficient model good for semantic similarity tasks\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Compute embeddings for all news article content in the training set\n",
    "embeddings = model.encode(train_news['content'].tolist(), show_progress_bar=True)"
   ],
   "id": "6573650a20b9cc7d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Build user profiles using the embeddings",
   "id": "eccda79e42b0649"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "user_embed_profiles = {}\n",
    "\n",
    "for _, row in train_behaviors.iterrows():\n",
    "    user = row['user_id']\n",
    "    history = row['history']\n",
    "    if pd.isna(history): continue\n",
    "    history_ids = [nid for nid in history.split() if nid in news_id_to_index]\n",
    "    if not history_ids: continue\n",
    "    indices = [news_id_to_index[nid] for nid in history_ids]\n",
    "    # Compute the mean embedding vector for the user's history\n",
    "    user_embedding = np.mean([embeddings[i] for i in indices], axis=0)\n",
    "    # Store the reshaped embedding (as a 2D array) in the dictionary, to be used later for cosine similarity\n",
    "    user_embed_profiles[user] = user_embedding.reshape(1, -1)"
   ],
   "id": "ba2fc4ded313de04"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Evaluate the model on the dev set",
   "id": "f61c80df0ab5d5d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "These four metrics—**AUC**, **MRR**, **nDCG\\@5**, and **nDCG\\@10**—are commonly used to evaluate the performance of recommendation or ranking systems.\n",
    "1. **AUC (Area Under the Curve)** measures the ability of the model to distinguish between positive and negative instances, with values closer to 1 indicating better discrimination.\n",
    "2. **MRR (Mean Reciprocal Rank)** evaluates how highly the first relevant item appears in the ranked list, rewarding models that place relevant items near the top.\n",
    "3. **nDCG\\@5** and **nDCG\\@10** (Normalized Discounted Cumulative Gain at positions 5 and 10) assess the quality of the top-ranked results by giving higher importance to relevant items appearing earlier in the list, normalized to account for the best possible ranking.\n"
   ],
   "id": "a6cc02cb8803bd3f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "dev_behaviors = pd.read_csv('MINDsmall_dev/behaviors.tsv', sep='\\t', header=None,\n",
    "                            names=['impression_id', 'user_id', 'time', 'history', 'impressions'])\n",
    "\n",
    "\n",
    "def mrr(scores, labels):\n",
    "    # Combine scores and labels, sort them in descending order of score\n",
    "    sorted_labels = [l for _, l in sorted(zip(scores, labels), reverse=True)]\n",
    "    for i, label in enumerate(sorted_labels):\n",
    "        if label == 1:\n",
    "            return 1.0 / (i + 1)\n",
    "    return 0.0\n",
    "\n",
    "def ndcg(scores, labels, k):\n",
    "    sorted_labels = [l for _, l in sorted(zip(scores, labels), reverse=True)][:k]\n",
    "    dcg = sum([1.0 / np.log2(i + 2) if l == 1 else 0 for i, l in enumerate(sorted_labels)])\n",
    "    ideal_dcg = sum([1.0 / np.log2(i + 2) for i in range(min(sum(labels), k))])\n",
    "    return dcg / ideal_dcg if ideal_dcg > 0 else 0.0\n",
    "\n",
    "def evaluate(dev_behaviors, news_id_to_index, news_vectors, user_profiles):\n",
    "    aucs, mrrs, ndcg5s, ndcg10s = [], [], [], []\n",
    "\n",
    "    for _, row in tqdm(dev_behaviors.iterrows(), total=len(dev_behaviors)):\n",
    "        user = row['user_id']\n",
    "        impressions = [imp.split('-') for imp in row['impressions'].split()]\n",
    "        news_ids = [nid for nid, _ in impressions if nid in news_id_to_index]\n",
    "        labels = [int(label) for nid, label in impressions if nid in news_id_to_index]\n",
    "\n",
    "        if not news_ids or user not in user_profiles or len(set(labels)) == 1:\n",
    "            continue\n",
    "\n",
    "        profile_vector = np.asarray(user_profiles[user])  # Convert to numpy array\n",
    "        candidate_idxs = [news_id_to_index[nid] for nid in news_ids]\n",
    "        candidate_vectors = news_vectors[candidate_idxs]\n",
    "        # Compute cosine similarity between user profile and each news candidate\n",
    "        sims = cosine_similarity(profile_vector, candidate_vectors).flatten()\n",
    "\n",
    "        aucs.append(roc_auc_score(labels, sims, multi_class='ovr'))\n",
    "        mrrs.append(mrr(sims, labels))\n",
    "        ndcg5s.append(ndcg(sims, labels, 5))\n",
    "        ndcg10s.append(ndcg(sims, labels, 10))\n",
    "\n",
    "    return {\n",
    "        'AUC': np.mean(aucs),\n",
    "        'MRR': np.mean(mrrs),\n",
    "        'nDCG@5': np.mean(ndcg5s),\n",
    "        'nDCG@10': np.mean(ndcg10s)\n",
    "    }"
   ],
   "id": "a2dcda5bf323a81"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Evaluate a baseline system (random reccomender)",
   "id": "b7be533f0a36bee3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def evaluate_random(dev_behaviors, news_id_to_index):\n",
    "    aucs, mrrs, ndcg5s, ndcg10s = [], [], [], []\n",
    "\n",
    "    for _, row in tqdm(dev_behaviors.iterrows(), total=len(dev_behaviors)):\n",
    "        impressions = [imp.split('-') for imp in row['impressions'].split()]\n",
    "        news_ids = [nid for nid, _ in impressions if nid in news_id_to_index]\n",
    "        labels = [int(label) for nid, label in impressions if nid in news_id_to_index]\n",
    "\n",
    "        if not news_ids or len(set(labels)) == 1:\n",
    "            continue\n",
    "\n",
    "        random_scores = [random.random() for _ in labels]\n",
    "\n",
    "        aucs.append(roc_auc_score(labels, random_scores))\n",
    "        mrrs.append(mrr(random_scores, labels))\n",
    "        ndcg5s.append(ndcg(random_scores, labels, 5))\n",
    "        ndcg10s.append(ndcg(random_scores, labels, 10))\n",
    "\n",
    "    return {\n",
    "        'AUC': np.mean(aucs),\n",
    "        'MRR': np.mean(mrrs),\n",
    "        'nDCG@5': np.mean(ndcg5s),\n",
    "        'nDCG@10': np.mean(ndcg10s)\n",
    "    }"
   ],
   "id": "135f3f9f26904df"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Compare results",
   "id": "d853f7747a3f5c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "cb_results = evaluate(dev_behaviors, news_id_to_index, news_vectors, user_profiles)\n",
    "rand_results = evaluate_random(dev_behaviors, news_id_to_index)\n",
    "embed_results = evaluate(dev_behaviors, news_id_to_index, embeddings, user_embed_profiles)\n",
    "\n",
    "# Display comparison\n",
    "print(\"Content-Based Recommender Performance:\")\n",
    "for metric, score in cb_results.items():\n",
    "    print(f\"{metric}: {score:.4f}\")\n",
    "\n",
    "print(\"\\nContent-Based Embedding Recommender Performance:\")\n",
    "for metric, score in embed_results.items():\n",
    "    print(f\"{metric}: {score:.4f}\")\n",
    "\n",
    "print(\"\\nRandom Recommender Performance:\")\n",
    "for metric, score in rand_results.items():\n",
    "    print(f\"{metric}: {score:.4f}\")"
   ],
   "id": "dc34187cea464794"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Combine results into a single dictionary for easy plotting\n",
    "all_results = {\n",
    "    \"Random\": rand_results,\n",
    "    \"Content-Based\": cb_results,\n",
    "    \"Embedding-Based\": embed_results    \n",
    "}\n",
    "\n",
    "# Round the results to 4 decimal places\n",
    "for model, metrics in all_results.items():\n",
    "    for metric, score in metrics.items():\n",
    "        all_results[model][metric] = round(score, 4)\n",
    "\n",
    "# Convert the results into a DataFrame for easier plotting\n",
    "results_df = pd.DataFrame(all_results).T\n",
    "results_df.reset_index(inplace=True)\n",
    "results_df.columns = ['Model', 'AUC', 'MRR', 'nDCG@5', 'nDCG@10']\n",
    "results_df = results_df.melt(id_vars='Model', var_name='Metric', value_name='Score')\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(12, 6))\n",
    "# Create a bar plot\n",
    "ax = sns.barplot(x='Metric', y='Score', hue='Model', data=results_df)\n",
    "ax.bar_label(ax.containers[0])\n",
    "ax.bar_label(ax.containers[1])\n",
    "ax.bar_label(ax.containers[2])\n",
    "# Set the title and labels\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.xlabel('Metrics')\n",
    "plt.ylabel('Score')\n",
    "# Show the plot\n",
    "plt.legend(title='Model')\n",
    "plt.show()"
   ],
   "id": "3d59cc1220b91b68"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
