{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import random\n",
    "from sklearn.linear_model import LinearRegression"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5a54cae31b9630a6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Step 1: Load and Explore the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "id": "1423d2360ebb59a2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "df_ratings = pd.read_csv(\"ml-latest-small/ratings.csv\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "94a565684743e4ec",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "df_ratings.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "80a7bf8e26ec6994",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "df_ratings.describe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b53cd1e17a28f512",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "df_movies = pd.read_csv(\"ml-latest-small/movies.csv\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "91c804c08b66c1a9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "df_movies.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ddc43d748558ed91",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Step 2: Analyze the Sparsity of the Rating Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c12bd7d6497e1c2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Construct a user-item interaction matrix where rows represent users and columns represent movies, with ratings as values."
   ]
  },
  {
   "cell_type": "code",
   "id": "5265f14b824737f3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "df_pivot = df_ratings.pivot(index=\"userId\", columns=\"movieId\", values=\"rating\")\n",
    "df_pivot.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "75212297187174f9",
   "metadata": {},
   "source": [
    "# Contare il numero di rating effettuati da ciascun utente\n",
    "ratings_per_user = df_pivot.notna().sum(axis=1)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(ratings_per_user, bins=len(ratings_per_user.unique()))\n",
    "plt.title('Distribuzione del numero di rating per utente')\n",
    "plt.xlabel('Numero di rating')\n",
    "plt.ylabel('Frequenza')\n",
    "plt.show()\n",
    "\n",
    "print(ratings_per_user.describe())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b45bee32c1e8e0e3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "####  Compute the sparsity of the matrix:"
   ]
  },
  {
   "cell_type": "code",
   "id": "696ac8462c991c96",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "num_of_ratings = len(df_ratings)\n",
    "user_movie_pairs = len(df_ratings)*len(df_movies)\n",
    "sparsity = 1 - num_of_ratings/user_movie_pairs\n",
    "sparsity"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3e615ef2a5f33b45",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Step 3: Handle Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8763db707b220e15",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Identify movies and users with the most missing ratings."
   ]
  },
  {
   "cell_type": "code",
   "id": "2ed9b7869bf922d0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Conta i valori mancanti (NaN) per ogni film\n",
    "missing_by_movie = df_pivot.isna().sum().sort_values(ascending=False)\n",
    "\n",
    "# Converte gli ID in titoli usando movie_dict\n",
    "movie_dict = pd.Series(df_movies.title.values, index=df_movies.movieId).to_dict()\n",
    "missing_by_movie.index = missing_by_movie.index.map(movie_dict)\n",
    "\n",
    "# Mostra i film con più rating mancanti\n",
    "print(missing_by_movie.head(10))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2576cae1f7dcbba8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Conta i valori mancanti per ogni utente\n",
    "missing_by_user = df_pivot.isna().sum(axis=1).sort_values(ascending=False)\n",
    "\n",
    "# Mostra gli utenti con più rating mancanti\n",
    "print(missing_by_user.head(10))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "720901cdf16f449d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Apply simple missing value imputation techniques:\n",
    "- Fill missing ratings with the movie’s average rating."
   ]
  },
  {
   "cell_type": "code",
   "id": "ed397eac57a0cf19",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Calcola la media di ogni film (colonne)\n",
    "movie_avg = df_pivot.mean(axis=0)\n",
    "\n",
    "# Riempie i valori NaN con la media del film\n",
    "df_movie_avg_filled = df_pivot.apply(lambda col: col.fillna(movie_avg[col.name]), axis=0)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ccb42595a48282ca",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- Fill missing ratings with the user’s average rating."
   ]
  },
  {
   "cell_type": "code",
   "id": "60b4c0217a3444eb",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Calcola la media di ogni utente (righe)\n",
    "user_avg = df_pivot.mean(axis=1)\n",
    "\n",
    "# Riempie i valori NaN con la media dell'utente\n",
    "df_user_avg_filled = df_pivot.apply(lambda row: row.fillna(user_avg[row.name]), axis=1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "195581b12fb65c10",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- Fill missing ratings with the global average rating."
   ]
  },
  {
   "cell_type": "code",
   "id": "9afa4320b9a044fc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Calcola la media globale\n",
    "global_avg = df_pivot.stack().mean() \n",
    "\n",
    "# Riempie i NaN con la media globale\n",
    "df_global_avg_filled = df_pivot.fillna(global_avg)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f69b8f829f8f25d1",
   "metadata": {},
   "source": [
    "chosen_df_filled = df_user_avg_filled"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4d90a8cb9b85adc2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Step 4: Week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7734a223c431d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Identify which kind of rating you have in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "id": "36e234caaa39b27e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Valori unici dei rating\n",
    "print(\"Uniques: \", df_ratings['rating'].unique())\n",
    "\n",
    "# Statistiche di base\n",
    "print(\"\\nStats: \\t\", df_ratings['rating'].describe())\n",
    "\n",
    "# Distribuzione dei rating\n",
    "plt.hist(df_ratings['rating'], bins=20)\n",
    "plt.title('Distribuzione dei rating')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Frequenza')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b8fa142b73439c72",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Verify the ratings have a long tail distribution"
   ]
  },
  {
   "cell_type": "code",
   "id": "4cb79ee81e78dc0a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "ratings_per_movie = df_pivot.count()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(ratings_per_movie.sort_values(ascending=False).values, label='Sorted Movie Ratings')\n",
    "plt.plot(ratings_per_movie.values, alpha=0.5, label=\"Movie Ratings\")\n",
    "\n",
    "plt.xlabel(\"Movies sorted by number of ratings\")\n",
    "plt.ylabel(\"Number of ratings\")\n",
    "plt.title(\"Long Tail Distribution of Movie Ratings\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "67c39f0ca404395c",
   "metadata": {},
   "source": [
    "#### Implement a Neighbourhood Based Collaborative Filtering exploiting a user based approach to identify the neighborhood."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616dae511a50ff50",
   "metadata": {},
   "source": [
    "#### 1. Pearson Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "id": "395f7aba2de37dd8",
   "metadata": {},
   "source": [
    "corr_matrix = df_pivot.T.corr(method=\"pearson\")\n",
    "mask = np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
    "df_pearson = (\n",
    "    corr_matrix.where(mask)\n",
    "    .stack()\n",
    "    .rename_axis([\"user_1\", \"user_2\"])\n",
    "    .reset_index(name=\"pearson_corr\")\n",
    ")\n",
    "\n",
    "display(df_pearson.head(10))\n",
    "display(df_pearson.describe())\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "151858424336d4f7",
   "metadata": {},
   "source": [
    "#### 2. Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "id": "e4500d3c61b3f602",
   "metadata": {},
   "source": [
    "# Calcolare la similarità del coseno tra gli utenti\n",
    "user_similarity = cosine_similarity(chosen_df_filled)\n",
    "# Creare un DataFrame di similarità\n",
    "user_similarity_df = pd.DataFrame(user_similarity, index=chosen_df_filled.index, columns=chosen_df_filled.index)\n",
    "print(\"Similarità tra utenti:\\n\", user_similarity_df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "904bcd64e991beb5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "def predict_rating(utente_target, film_target, user_similarity_df=user_similarity_df):\n",
    "    if utente_target not in chosen_df_filled.index:\n",
    "        raise ValueError(f\"L'utente {utente_target} non esiste nel dataset.\")\n",
    "    if film_target not in chosen_df_filled.columns:\n",
    "        raise ValueError(f\"Il film {film_target} non esiste nel dataset.\")\n",
    "\n",
    "    # Ordinare gli utenti in base alla similarità (escludendo l'utente stesso)\n",
    "    neighborhood = user_similarity_df[utente_target].drop(utente_target).sort_values(ascending=False)\n",
    "    print(f\"\\nVicinato dei 5 utenti di [{utente_target}]:\\n\", neighborhood[:5])\n",
    "\n",
    "    # Prendere solo i rating dei vicini per quel film\n",
    "    ratings_from_neighbors = chosen_df_filled.loc[neighborhood.index, film_target]\n",
    "\n",
    "    # Ponderare i rating in base alla similarità\n",
    "    weighted_sum = sum(rating * neighborhood[utente] for utente, rating in ratings_from_neighbors.items())\n",
    "    similarity_sum = sum(neighborhood[utente] for utente in ratings_from_neighbors.index)\n",
    "\n",
    "    # Previsione del rating (evitare divisione per 0)\n",
    "    if similarity_sum != 0:\n",
    "        predicted_rating = weighted_sum / similarity_sum\n",
    "    else:\n",
    "        predicted_rating = np.nan\n",
    "\n",
    "    print(f\"\\nRating previsto per utente [{utente_target}] su {movie_dict[film_target]}: {predicted_rating:.2f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c4bd2257e95685d4",
   "metadata": {},
   "source": [
    "chosen_user = random.choice(chosen_df_filled.index)\n",
    "chosen_film = random.choice(chosen_df_filled.columns)\n",
    "\n",
    "predict_rating(chosen_user, chosen_film, user_similarity_df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "215ea546c4c5da92",
   "metadata": {},
   "source": [
    "####  Handle the personal bias by weighting the rating data present in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "id": "c61d9fa46836a18e",
   "metadata": {},
   "source": [
    "# Centrare i dati sottraendo la media dei rating per ogni utente\n",
    "user_means = chosen_df_filled.mean(axis=1)\n",
    "df_centered = chosen_df_filled.sub(user_means, axis=0)\n",
    "\n",
    "# Ricalcolare la similarità del coseno sui dati centrati\n",
    "user_similarity = cosine_similarity(df_centered)\n",
    "user_similarity_no_bias_df = pd.DataFrame(user_similarity, index=chosen_df_filled.index, columns=chosen_df_filled.index)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "af4f8bd2e987dc92",
   "metadata": {},
   "source": [
    "predict_rating(chosen_user, chosen_film, user_similarity_no_bias_df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9dca303824796d31",
   "metadata": {},
   "source": [
    "#### Apply a Discount Factor in the calculation of similarity"
   ]
  },
  {
   "cell_type": "code",
   "id": "be9ae14e9da2ed88",
   "metadata": {},
   "source": [
    "# Conteggio degli item in comune tra due utenti\n",
    "common_items_count = df_pivot.notna().astype(int).dot(df_pivot.notna().astype(int).T)\n",
    "\n",
    "# Discount factor: più bassa è la sovrapposizione, più forte è lo sconto\n",
    "min_common_items = 15  # Visto che minimo un utente ha recensito 20 film, scelto 15 come n# film comuni\n",
    "discount_factor = common_items_count / (common_items_count + min_common_items)\n",
    "\n",
    "# Applicare il discount factor alla similarità\n",
    "user_similarity_discounted = user_similarity_no_bias_df * discount_factor\n",
    "user_similarity_discounted_df = pd.DataFrame(user_similarity_discounted, index=chosen_df_filled.index, columns=chosen_df_filled.index)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2ce150abcc5bc2e4",
   "metadata": {},
   "source": [
    "predict_rating(chosen_user, chosen_film, user_similarity_discounted_df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9eeaa69409358a8",
   "metadata": {},
   "source": "####  Integrate a strategy to handle the Long Tail Problem"
  },
  {
   "cell_type": "code",
   "id": "e9cce3ebf7bf658",
   "metadata": {},
   "source": [
    "# 1. Calcolare il numero di utenti che hanno visto ogni film (non-NaN)\n",
    "num_users_per_movie = df_pivot.notna().sum()\n",
    "\n",
    "# 2. Calcolare l'Inverse User Frequency (IUF) per ciascun film\n",
    "total_users = df_pivot.shape[0]  # Numero totale di utenti (righe)\n",
    "iuf = np.log(total_users / num_users_per_movie)\n",
    "\n",
    "# 3. Creare una matrice IUF per ogni film (con le colonne in IUF)\n",
    "iuf_matrix = df_pivot.copy()  # Copia del dataframe per aggiungere l'IUF\n",
    "for movie in df_pivot.columns:\n",
    "    iuf_matrix[movie] = df_pivot[movie] * iuf[movie]  # Ponderare il rating per l'IUF\n",
    "\n",
    "print(iuf_matrix.info(), \"\\n\")\n",
    "print(iuf_matrix.stack().describe())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e7812d4d2bd6a68c",
   "metadata": {},
   "source": [
    "# Ricalcolo similarity con IUF\n",
    "iuf_matrix_filled = iuf_matrix.apply(lambda row: row.fillna(row.mean()), axis=1)\n",
    "cos_iuf_matrix = cosine_similarity(iuf_matrix_filled)\n",
    "\n",
    "# Creare un DataFrame per visualizzare la matrice di similarità\n",
    "iuf_similarity_df = pd.DataFrame(cos_iuf_matrix, index=iuf_matrix.index, columns=iuf_matrix.index)\n",
    "\n",
    "# Visualizzare la matrice di similarità\n",
    "print(iuf_similarity_df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e7ee3c8406ee6ce1",
   "metadata": {},
   "source": [
    "predict_rating(chosen_user, chosen_film, iuf_similarity_df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f676e6a9",
   "metadata": {},
   "source": [
    "#  User-Based Nearest Neighbour Regression Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "id": "1e361738",
   "metadata": {},
   "source": [
    "# Function to retrieve top-k neighbors for a given user\n",
    "def get_top_k_neighbors(u, similarity_df, k=10):\n",
    "    \"\"\"\n",
    "    Retrieves the top-k most similar users for a given user.\n",
    "\n",
    "    Parameters:\n",
    "    - u (int): The ID of the target user for whom we want to find similar users.\n",
    "    - similarity_df (pd.DataFrame): DataFrame containing pairwise user similarity scores.\n",
    "    - k (int, optional): Number of top neighbors to return (default is 10).\n",
    "\n",
    "    Returns:\n",
    "    - pd.Index: Index of the top-k most similar users.\n",
    "    \"\"\"\n",
    "    # Extract the similarity row corresponding to user 'u', excluding the user itself\n",
    "    sim_row = similarity_df.loc[u].drop(u, errors='ignore')\n",
    "    \n",
    "    # Sort similarities in descending order and select the top-k users\n",
    "    top_k = sim_row.nlargest(k).index  \n",
    "    return top_k\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def train_regression_for_user(u, df_pivot, user_avg, neighbors, min_common=2, verbose=False):\n",
    "    \"\"\"\n",
    "    Trains a linear regression model for a given user to predict their ratings \n",
    "    based on their neighbors' ratings.\n",
    "\n",
    "    Returns:\n",
    "      - w_{vu} (np.ndarray): The vector of regression weights for the user's neighbors.\n",
    "      - mse (float): The mean squared error (MSE) of the model on the training set.\n",
    "\n",
    "    Regression equation:\n",
    "        (r_{uj} - mu_u) = sum_v [ w_{vu} * (r_{vj} - mu_v) ]\n",
    "    for all items j that user u has rated.\n",
    "\n",
    "    Parameters:\n",
    "    - u (int): The target user ID for whom the regression model is being trained.\n",
    "    - df_pivot (pd.DataFrame): Pivot table of user-item ratings with NaN for missing values.\n",
    "    - user_avg (pd.Series): Series containing the average rating for each user.\n",
    "    - neighbors (list): List of neighbor user IDs to be used as features in the regression model.\n",
    "    - min_common (int, optional): Minimum number of rated items required to train the model (default = 2).\n",
    "    - verbose (bool, optional): If True, prints additional information about the training process (default = False).\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray: Array of learned weights (one for each neighbor).\n",
    "    - float: Mean squared error (MSE) on the training set. Returns np.nan if there are insufficient training points.\n",
    "    \"\"\"\n",
    "\n",
    "    # List of items rated by user u\n",
    "    user_ratings = df_pivot.loc[u]\n",
    "    items_rated_by_u = user_ratings[user_ratings.notna()].index\n",
    "\n",
    "    # Target vector Y (centered ratings) and feature matrix X\n",
    "    X = []  # Each row corresponds to a rated item with features from neighbors\n",
    "    Y = []  # Each element corresponds to the target value for that item\n",
    "\n",
    "    # Mean rating of user u\n",
    "    mu_u = user_avg[u]\n",
    "\n",
    "    # Build X (neighbor features) and Y (target) for each rated item\n",
    "    for item_j in items_rated_by_u:\n",
    "        r_uj = df_pivot.loc[u, item_j]  # User u's rating for item j\n",
    "        y_j = r_uj - mu_u               # Centered rating (target)\n",
    "        \n",
    "        row_features = []\n",
    "        # Build features for each neighbor's centered rating\n",
    "        for v in neighbors:\n",
    "            r_vj = df_pivot.loc[v, item_j]  # Neighbor v's rating for item j\n",
    "            if pd.isna(r_vj):  \n",
    "                # If the neighbor hasn't rated item j, use 0 (or fallback to their mean)\n",
    "                row_features.append(0.0)\n",
    "            else:\n",
    "                mu_v = user_avg[v]  # Mean rating of neighbor v\n",
    "                row_features.append(r_vj - mu_v)\n",
    "\n",
    "        X.append(row_features)\n",
    "        Y.append(y_j)\n",
    "\n",
    "    # Convert to numpy arrays for regression\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "\n",
    "    # If there are too few data points, return zero weights and np.nan MSE\n",
    "    if len(Y) < min_common:\n",
    "        if verbose:\n",
    "            print(f\"[User {u}] Too few items for regression: {len(Y)} < {min_common}\")\n",
    "        return np.zeros(len(neighbors)), np.nan\n",
    "\n",
    "    # Train a linear regression model without an intercept\n",
    "    model = LinearRegression(fit_intercept=False)\n",
    "    model.fit(X, Y)\n",
    "\n",
    "    # Calculate MSE on the training set\n",
    "    Y_pred = model.predict(X)\n",
    "    mse = np.mean((Y_pred - Y)**2)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[User {u}] MSE: {mse:.4f}\")\n",
    "\n",
    "    return model.coef_, mse"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "661bb45f32eda87d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "user_regression_models = {}  # Dictionary to store regression models for each user\n",
    "all_mse = []  # List to track MSE values across all users\n",
    "\n",
    "# Number of top similar users to retain\n",
    "k = 40\n",
    "\n",
    "# Iterate over all users in the pivot table\n",
    "for u in df_pivot.index:\n",
    "    # Identify the top k neighbors for user u\n",
    "    neighbors_u = get_top_k_neighbors(u, user_similarity_df, k)\n",
    "    \n",
    "    # Train the regression model for user u\n",
    "    w_u, mse_u = train_regression_for_user(\n",
    "        u=u,                      # Current user ID\n",
    "        df_pivot=df_pivot,        # Pivot table with user-item ratings\n",
    "        user_avg=user_avg,        # Series with average user ratings\n",
    "        neighbors=neighbors_u,    # Identified top k neighbors\n",
    "        min_common=2,             # Minimum items in common for valid training\n",
    "        verbose=True              # Display MSE for each user during training\n",
    "    )\n",
    "    \n",
    "    # Store the trained model's results\n",
    "    user_regression_models[u] = {\n",
    "        'neighbors': neighbors_u,  # List of neighbor IDs\n",
    "        'weights': w_u             # Corresponding regression weights\n",
    "    }\n",
    "\n",
    "    # Collect the MSE value for performance analysis\n",
    "    all_mse.append(mse_u)\n",
    "\n",
    "# Compute and display the overall average MSE (ignoring NaN values)\n",
    "all_mse = [m for m in all_mse if not np.isnan(m)]\n",
    "if all_mse:\n",
    "    print(f\"\\nAverage MSE across all users: {np.mean(all_mse):.4f}\")\n",
    "else:\n",
    "    print(\"No MSE available (too many users with too few ratings?).\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a3fc0258d3bef87a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](images/img.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "762f480bb70dfad5"
  },
  {
   "cell_type": "markdown",
   "source": "![](images/img_2.png)",
   "metadata": {
    "collapsed": false
   },
   "id": "d026b1877b87a536"
  },
  {
   "cell_type": "code",
   "source": [
    "def predict_rating_regression(u, j, df_pivot, user_avg, user_regression_models):\n",
    "    \"\"\"\n",
    "    Predicts the rating of user u for item j using the regression model's learned weights.\n",
    "\n",
    "    Parameters:\n",
    "    - u (int): ID of the user for whom the rating is predicted.\n",
    "    - j (int): ID of the item (movie) to predict the rating for.\n",
    "    - df_pivot (pd.DataFrame): Pivot table with users as rows, items as columns, and values as ratings.\n",
    "    - user_avg (pd.Series): Mean rating for each user.\n",
    "    - user_regression_models (dict): Dictionary containing for each user:\n",
    "        - 'neighbors': List of neighbors.\n",
    "        - 'weights': Vector of learned regression weights.\n",
    "\n",
    "    Returns:\n",
    "    - float: The predicted rating for user u on item j, or np.nan if the model is unavailable.\n",
    "    \"\"\"\n",
    "\n",
    "    # If the user has no regression model, return np.nan\n",
    "    if u not in user_regression_models:\n",
    "        return np.nan\n",
    "\n",
    "    # Retrieve neighbors and learned weights\n",
    "    neighbors = user_regression_models[u]['neighbors']\n",
    "    weights = user_regression_models[u]['weights']\n",
    "    mu_u = user_avg[u]  # Mean rating of user u\n",
    "\n",
    "    # Build the weighted contribution sum from neighbors\n",
    "    contribution_sum = 0.0\n",
    "    for w_vu, v in zip(weights, neighbors):\n",
    "        # Get neighbor v's rating for item j\n",
    "        r_vj = df_pivot.loc[v, j]\n",
    "        # If neighbor v hasn't rated item j, use their mean rating as a fallback\n",
    "        if pd.isna(r_vj):\n",
    "            r_vj = user_avg[v]\n",
    "\n",
    "        mu_v = user_avg[v]  # Mean rating of neighbor v\n",
    "        contribution_sum += w_vu * (r_vj - mu_v)  # Weighted contribution\n",
    "\n",
    "    # Return the predicted rating using the formula: r_{uj} = mu_u + sum_v [w_{vu} * (r_{vj} - mu_v)]\n",
    "    return mu_u + contribution_sum"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "90ea42d7e57dae26",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b7f3c74e",
   "metadata": {},
   "source": [
    "# Predict the rating for the chosen user and item\n",
    "pred = predict_rating_regression(chosen_user, chosen_film, df_pivot, user_avg, user_regression_models)\n",
    "print(f\"Predicted rating for user [{chosen_user}] on item {movie_dict[chosen_film]}: {pred:.2f}\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
