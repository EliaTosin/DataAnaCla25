{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import random\n",
    "random.seed(42)"
   ],
   "id": "e8f99a01390f6270"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "DATA_PATH = \"../../ml-latest-small\"\n",
    "\n",
    "ratings = pd.read_csv(os.path.join(DATA_PATH, \"ratings.csv\"))\n",
    "movies = pd.read_csv(os.path.join(DATA_PATH, \"movies.csv\"))\n",
    "tags = pd.read_csv(os.path.join(DATA_PATH, \"tags.csv\"))"
   ],
   "id": "c6c31514452fb88f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Naive Bayes CF Model Based (Week6)",
   "id": "7019d53115e25098"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_ecommerce= pd.read_json('1_ecommerce.jsonl', lines=True)\n",
    "\n",
    "# for each session, we will create a list of items that the user has clicked on, removing duplicates\n",
    "clicks_items_list = []\n",
    "carts_items_list = []\n",
    "orders_items_list = []\n",
    "\n",
    "for events in df_ecommerce.events:\n",
    "    clicks = []\n",
    "    carts = []\n",
    "    orders = []\n",
    "    for e in events:\n",
    "        if e['type'] == 'clicks':\n",
    "            clicks.append(e['aid'])\n",
    "        if e['type'] == 'carts':\n",
    "            carts.append(e['aid'])\n",
    "        if e['type'] == 'orders':\n",
    "            orders.append(e['aid'])\n",
    "\n",
    "    clicks_items_list.append(list(clicks))\n",
    "    carts_items_list.append(list(carts))\n",
    "    orders_items_list.append(list(orders))\n",
    "\n",
    "df_ecommerce['clicks'] = clicks_items_list\n",
    "df_ecommerce['carts'] = carts_items_list\n",
    "df_ecommerce['orders'] = orders_items_list\n",
    "df = df_ecommerce.drop(columns=[\"events\"], axis=1)\n",
    "\n",
    "clicks_length = df_ecommerce['clicks'].apply(len)\n",
    "carts_length = df_ecommerce['carts'].apply(len)\n",
    "orders_length = df_ecommerce['orders'].apply(len)\n",
    "\n",
    "# takes only the session with items length higher than 20\n",
    "df_truncated = df[df['clicks'].apply(lambda x: len(x) > 10)]\n",
    "df = df_truncated.copy()\n",
    "# redefine index\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Explode each column (clicks, carts, orders)\n",
    "df_clicks = df[['session', 'clicks']].explode('clicks').rename(columns={'clicks': 'item'}).dropna(subset=['item'])\n",
    "df_carts = df[['session', 'carts']].explode('carts').rename(columns={'carts': 'item'}).dropna(subset=['item'])\n",
    "df_orders = df[['session', 'orders']].explode('orders').rename(columns={'orders': 'item'}).dropna(subset=['item'])\n",
    "\n",
    "# Concatenate the exploded dataframes\n",
    "df_concat = pd.concat([df_clicks, df_carts, df_orders])\n",
    "\n",
    "# Create a new column for each category indicating whether the item is present in that category\n",
    "df_concat['click'] = df_concat['item'].isin(df_clicks['item']).astype(int)\n",
    "df_concat['cart'] = df_concat['item'].isin(df_carts['item']).astype(int)\n",
    "df_concat['order'] = df_concat['item'].isin(df_orders['item']).astype(int)\n",
    "\n",
    "# Drop duplicates based on session and item\n",
    "df_concat = df_concat.drop_duplicates(subset=['session', 'item'])"
   ],
   "id": "ef990b1d18a15e78"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def sgd_matrix_factorization(df, k=10, alpha=0.01, lambda_reg=0.1, num_epochs=20, w_click=1, w_cart=3, w_order=5, test_size=0.2, validation_size=0.1):\n",
    "    # Map session (users) and items to consecutive indices\n",
    "    users = {u: i for i, u in enumerate(df['session'].unique())}\n",
    "    items = {i: j for j, i in enumerate(df['item'].unique())}\n",
    "\n",
    "    num_users = len(users)\n",
    "    num_items = len(items)\n",
    "\n",
    "    # Initialize U, V, and biases\n",
    "    U = np.random.rand(num_users, k)\n",
    "    V = np.random.rand(num_items, k)\n",
    "    b_u = np.zeros(num_users)\n",
    "    b_i = np.zeros(num_items)\n",
    "    b = 0  # Global bias\n",
    "\n",
    "    # Create (u, i, r_ui) tuples for all interactions\n",
    "    data = []\n",
    "\n",
    "    # iter throw all the rows of the dataframe\n",
    "    # and create a list of tuples (user, item, rating)\n",
    "    # Assign ratings to interactions\n",
    "    for _, row in df.iterrows():\n",
    "        u = users[row['session']]\n",
    "        i = items[row['item']]\n",
    "\n",
    "        # if this user has perform an action on this item, we assign a rating based on the action\n",
    "        # type starting from the most important one\n",
    "        # (order > cart > click)\n",
    "        if row['order'] > 0:\n",
    "            r_ui = w_order\n",
    "        elif row['cart'] > 0:\n",
    "            r_ui = w_cart\n",
    "        elif row['click'] > 0:\n",
    "            r_ui = w_click\n",
    "        else:\n",
    "            continue  # Skip interactions with no recorded action\n",
    "\n",
    "        data.append((u, i, r_ui))\n",
    "\n",
    "    # Split the data into training, validation, and test sets\n",
    "    train_data, test_data = train_test_split(data, test_size=test_size, random_state=42)\n",
    "    train_data, val_data = train_test_split(train_data, test_size=validation_size, random_state=42)"
   ],
   "id": "bf5ae7de393eda7f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Unconstrained Matrix Factorisation based Collaborative Filtering (week 7)",
   "id": "54d40534a6a27bac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_ratings = pd.read_csv(\"../ml-latest-small/ratings.csv\")\n",
    "\n",
    "df_ratings.loc[df_ratings['rating'] <= 2, 'rating_ordinal'] = \"Don't like\"\n",
    "df_ratings.loc[(df_ratings['rating'] > 2) & (df_ratings['rating'] <= 4), 'rating_ordinal'] = \"Like\"\n",
    "df_ratings.loc[df_ratings['rating'] > 4, 'rating_ordinal'] = \"Really like\"\n",
    "\n",
    "ratings_ordinals = df_ratings['rating_ordinal'].unique()\n",
    "\n",
    "df_movies = pd.read_csv(\"../ml-latest-small/movies.csv\")\n",
    "\n",
    "df_user_movie_ratings = df_ratings.pivot(\n",
    "    index='userId',\n",
    "    columns='movieId',\n",
    "    values='rating_ordinal'\n",
    ")\n",
    "df_user_movie_ratings.head()\n",
    "\n",
    "all_films = df_ratings[\"movieId\"].unique()\n",
    "\n",
    "# Crea una Serie booleana: True dove il rating è mancante\n",
    "missing = df_user_movie_ratings.isnull()\n",
    "\n",
    "# Trasforma in formato \"long\" (una riga per ogni cella)\n",
    "missing = missing.stack()\n",
    "\n",
    "# Filtra solo le celle in cui il rating è mancante\n",
    "missing = missing[missing].reset_index()\n",
    "missing.columns = ['userId', 'movieId', 'is_missing']\n",
    "\n",
    "# Aggiungi la colonna 'flag' con valore False\n",
    "df_ratings_missing = missing[['userId', 'movieId']].copy()"
   ],
   "id": "ed48ad65a701138d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# itera per tutte le valutazioni manacanti\n",
    "for row in df_ratings_missing.itertuples():\n",
    "    userId = row.userId\n",
    "    movieId = row.movieId\n",
    "\n",
    "    # film seen from the user\n",
    "    films_seen = df_user_movie_ratings.loc[userId].dropna().index\n",
    "\n",
    "    for category in ratings_ordinals:\n",
    "        # P(r31 = 1)\n",
    "\n",
    "        df_current_movie = df_ratings[(df_ratings['movieId'] == movieId)]\n",
    "        df_current_movie_category = df_current_movie[(df_current_movie['rating_ordinal'] == category)]\n",
    "        users_that_voted_current_movies_with_this_category = df_current_movie_category['userId'].unique()\n",
    "\n",
    "        p_r31 = len(df_current_movie_category) / len(df_current_movie)\n",
    "\n",
    "        probs = [p_r31]\n",
    "        for film_seen in films_seen:\n",
    "            # P(r32 = 1 | r31 = 1)\n",
    "            # prendo la valutazione che l'utente ha dato al film visto\n",
    "            assigned_rating = df_user_movie_ratings.loc[userId, film_seen]\n",
    "\n",
    "            # cerco tutti gli utenti che hanno votato il film visto come l'utente corrente è che hanno valutato il\n",
    "            # film corrente con la stessa categoria\n",
    "\n",
    "            df_seen_movie = df_ratings[(df_ratings['movieId'] == film_seen)]\n",
    "            df_seen_movie_category = df_seen_movie[df_seen_movie['userId'].isin(users_that_voted_current_movies_with_this_category)]\n",
    "\n",
    "            # di quelli, cerco quanti hanno votato il film che l'utente ha visto con la stessa categoria\n",
    "            df_seen_movie_category = df_seen_movie_category[df_seen_movie_category['rating_ordinal'] == assigned_rating]\n",
    "\n",
    "            # if no one has voted the movie with the same category, we skip it\n",
    "            if len(df_seen_movie_category) == 0:\n",
    "                continue\n",
    "\n",
    "            probs.append(len(df_seen_movie_category) / len(df_seen_movie))\n",
    "\n",
    "        # P(r32 = 1 | r31 = 1) * P(r31 = 1)\n",
    "        prob = np.prod(probs)\n",
    "        df_ratings_missing.loc[(df_ratings_missing['userId'] == userId) & (df_ratings_missing['movieId'] == movieId), category] = prob"
   ],
   "id": "6199cf520705d832"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "def compute_missing_prob(row):\n",
    "    userId = row['userId']\n",
    "    movieId = row['movieId']\n",
    "    # Film che l'utente ha visto\n",
    "    films_seen = df_user_movie_ratings.loc[userId].dropna().index.tolist()\n",
    "\n",
    "    # Dizionario che conterrà il risultato per ogni categoria\n",
    "    result = {}\n",
    "\n",
    "    # Per ciascuna categoria (rating) da considerare\n",
    "    for category in ratings_ordinals:\n",
    "        # Calcola P(r(movieId) = category)\n",
    "        df_current_movie = df_ratings[df_ratings['movieId'] == movieId]\n",
    "        df_current_movie_category = df_current_movie[df_current_movie['rating_ordinal'] == category]\n",
    "        # Gestione di eventuale divisione per zero:\n",
    "        if len(df_current_movie) == 0:\n",
    "            p_r31 = 0\n",
    "        else:\n",
    "            p_r31 = len(df_current_movie_category) / len(df_current_movie)\n",
    "\n",
    "        # Lista delle probabilità da moltiplicare\n",
    "        probs = [p_r31]\n",
    "\n",
    "        # Itera sui film che l'utente ha visto\n",
    "        for film_seen in films_seen:\n",
    "            assigned_rating = df_user_movie_ratings.loc[userId, film_seen]\n",
    "            # Filtra i voti del film visto\n",
    "            df_seen_movie = df_ratings[df_ratings['movieId'] == film_seen]\n",
    "            # Limita agli utenti che hanno votato il film mancante con 'category'\n",
    "            users_voted_current = df_current_movie_category['userId'].unique()\n",
    "            df_seen_movie_category = df_seen_movie[df_seen_movie['userId'].isin(users_voted_current)]\n",
    "            # Filtra in base al rating assegnato dall'utente al film visto\n",
    "            df_seen_movie_category = df_seen_movie_category[df_seen_movie_category['rating_ordinal'] == assigned_rating]\n",
    "\n",
    "            # Se non ci sono voti, puoi decidere se saltare il film oppure applicare uno smoothing (qui si salta)\n",
    "            if len(df_seen_movie) == 0:\n",
    "                p_cond = 1  # oppure 0 oppure applicare smoothing\n",
    "            else:\n",
    "                if len(df_seen_movie_category) == 0:\n",
    "                    continue\n",
    "                p_cond = len(df_seen_movie_category) / len(df_seen_movie)\n",
    "            probs.append(p_cond)\n",
    "\n",
    "        # Il prodotto delle probabilità\n",
    "        result[category] = np.prod(probs)\n",
    "\n",
    "    return pd.Series(result)\n",
    "\n",
    "# Applica la funzione a df_ratings_missing\n",
    "df_ratings_missing[ratings_ordinals] = df_ratings_missing.progress_apply(compute_missing_prob, axis=1)"
   ],
   "id": "15462e57361ac371"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Bayes Classification Content-based (week 8)",
   "id": "f5debd8f94280067"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## 🧹 Preprocess Movie Metadata\n",
    "tags_agg = tags.groupby(\"movieId\")[\"tag\"].apply(lambda x: \" \".join(x)).reset_index()\n",
    "movies = movies.merge(tags_agg, on=\"movieId\", how=\"left\")\n",
    "movies[\"tag\"] = movies[\"tag\"].fillna(\"\")\n",
    "movies[\"content\"] = movies[\"genres\"].str.replace(\"|\", \" \") + \" \" + movies[\"tag\"]"
   ],
   "id": "2113ce104faf20a9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 1. User-Specific Naive Bayes Recommender",
   "id": "4e800e705b0a614e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Clean the title by removing the year in parentheses\n",
    "def clean_title(title):\n",
    "    return re.sub(r'\\s*\\(\\d{4}\\)', '', title)\n",
    "\n",
    "metadata = movies[[\"movieId\", \"title\", \"genres\"]]\n",
    "metadata.loc[:, 'title'] = metadata['title'].apply(clean_title)\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "genres_encoded = mlb.fit_transform(metadata['genres'].str.split('|'))\n",
    "# Create a DataFrame with the encoded genres\n",
    "genres_df = pd.DataFrame(genres_encoded, columns=mlb.classes_)\n",
    "# Concatenate the original metadata with the encoded genres\n",
    "metadata = pd.concat([metadata[['movieId', 'title']], genres_df], axis=1)\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "\n",
    "# Create a preprocessor that transforms the movie metadata:\n",
    "# - Applies TF-IDF vectorization to the cleaned 'title' column to extract textual features.\n",
    "# - Passes through the binary genre columns (already transformed by MultiLabelBinarizer).\n",
    "# - Drops any remaining columns that are not explicitly selected.\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('tfidf', tfidf, 'title'),\n",
    "        ('genres', 'passthrough', genres_df.columns)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")"
   ],
   "id": "3c899a8aa448d3a3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def predict_single_movie(user_id, movie_id):\n",
    "    # Step 1: Prepare user data\n",
    "    user_ratings = ratings[ratings['userId'] == user_id]\n",
    "    user_data = pd.merge(user_ratings, metadata, on='movieId')\n",
    "\n",
    "    # Step 2: Create binary labels\n",
    "    user_data['label'] = user_data['rating'].apply(lambda r: 1 if r >= 4 else (0 if r <= 2 else None))\n",
    "    user_data = user_data.dropna(subset=['label'])\n",
    "    user_data['label'] = user_data['label'].astype(int)\n",
    "\n",
    "    if user_data.empty:\n",
    "        print(\"User has insufficient data.\")\n",
    "        return None\n",
    "\n",
    "    # Step 3: Train the model\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', MultinomialNB())\n",
    "    ])\n",
    "\n",
    "    X_train = user_data.drop(columns=['userId', 'rating', 'label', 'timestamp'])\n",
    "    y_train = user_data['label']\n",
    "\n",
    "    display(X_train.head())\n",
    "\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Step 4: Check if the movie has already been watched\n",
    "    if movie_id in user_ratings['movieId'].values:\n",
    "        print(\"The movie has already been watched by the user.\")\n",
    "        return None\n",
    "\n",
    "    # Step 5: Extract features of the requested movie\n",
    "    movie_row = metadata[metadata['movieId'] == movie_id]\n",
    "    display(movie_row)\n",
    "\n",
    "    if movie_row.empty:\n",
    "        print(\"Movie ID not found in the metadata.\")\n",
    "        return None\n",
    "\n",
    "    input_cols = list(X_train.columns)\n",
    "    movie_features = movie_row[input_cols]\n",
    "\n",
    "    # Step 6: Predict the probability of liking the movie\n",
    "    probs = pipeline.predict_proba(movie_features)[0]  # P(liked | features)\n",
    "    if len(probs) < 2:\n",
    "        print(f\"The model for user {user_id} has seen only one class (not like/like).\")\n",
    "        return None\n",
    "    else:\n",
    "        prob = probs[1]\n",
    "\n",
    "    title = movie_row['title'].values[0]\n",
    "\n",
    "    return {\n",
    "        'movieId': movie_id,\n",
    "        'title': title,\n",
    "        'score': prob,\n",
    "        'recommended': prob >= 0.5\n",
    "    }"
   ],
   "id": "621d69e8f2d31476"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "chosen_user = random.choice(ratings['userId'].unique())\n",
    "chosen_film = random.choice(metadata['movieId'].unique())\n",
    "\n",
    "predict_single_movie(user_id=chosen_user, movie_id=chosen_film)"
   ],
   "id": "ad8b0d9b5380540c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 2. Global Content-Based Recommender (Single Model for All Users)",
   "id": "14fffcebd321f853"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "metadata_with_tags = metadata.merge(movies[[\"movieId\", \"tag\"]], on='movieId', how='left')\n",
    "\n",
    "preprocessor_global = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('tfidf', tfidf, 'title'),\n",
    "        ('genres', 'passthrough', genres_df.columns),\n",
    "        ('tfidf_tag', tfidf, 'tag')\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")"
   ],
   "id": "1d8958d2b6dddc1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train_global_model():\n",
    "    # Step 1: Prepare data\n",
    "    data = pd.merge(ratings, metadata_with_tags, on='movieId')\n",
    "\n",
    "    # Step 2: Create binary labels\n",
    "    data['label'] = data['rating'].apply(lambda r: 1 if r >= 4 else (0 if r <= 2 else None))\n",
    "    data = data.dropna(subset=['label'])\n",
    "    data['label'] = data['label'].astype(int)\n",
    "\n",
    "    # Step 3: Train the model\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor_global),\n",
    "        ('classifier', MultinomialNB())\n",
    "    ])\n",
    "\n",
    "    X = data.drop(columns=['userId', 'movieId', 'rating', 'label', 'timestamp'])\n",
    "    y = data['label']\n",
    "\n",
    "    display(X.head())\n",
    "\n",
    "    model = pipeline.fit(X, y)\n",
    "    return model, X.columns\n",
    "\n",
    "model_global, train_columns = train_global_model()\n",
    "\n",
    "def recommend_global(user_id, movie_id):\n",
    "    # Step 4: Check if the movie has already been watched\n",
    "    user_ratings = ratings[ratings['userId'] == user_id]\n",
    "    if movie_id in user_ratings['movieId'].values:\n",
    "        print(\"The movie has already been watched by the user.\")\n",
    "        return None\n",
    "\n",
    "    # Step 5: Extract features of the requested movie\n",
    "    movie_row = metadata_with_tags[metadata_with_tags['movieId'] == movie_id]\n",
    "    display(movie_row)\n",
    "\n",
    "    if movie_row.empty:\n",
    "        print(\"Movie ID not found in the metadata.\")\n",
    "        return None\n",
    "    movie_features = movie_row[train_columns]\n",
    "\n",
    "    # Step 6: Predict the probability of liking the movie\n",
    "    probs = model_global.predict_proba(movie_features)[0]  # P(liked | features)\n",
    "    if len(probs) < 2:\n",
    "        print(f\"The model for user {user_id} has seen only one class (not like/like).\")\n",
    "        return None\n",
    "    else:\n",
    "        prob = probs[1]\n",
    "\n",
    "    title = movie_row['title'].values[0]\n",
    "\n",
    "    return {\n",
    "        'movieId': movie_id,\n",
    "        'title': title,\n",
    "        'score': prob,\n",
    "        'recommended': prob >= 0.5\n",
    "    }"
   ],
   "id": "90a58090827be11a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "recommend_global(user_id=chosen_user, movie_id=chosen_film)",
   "id": "207a8ae7c2df056d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Evaluation",
   "id": "69f21698b1b4497a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Importa le funzioni di valutazione dal modulo esterno\n",
    "from eval_functions import (\n",
    "    evaluate_rating_predictions,\n",
    "    analyze_long_tail_effect,\n",
    "    plot_avg_error_by_genre,\n",
    "    plot_avg_error_by_popularity,\n",
    "    evaluate_ranking_spearman,\n",
    "    evaluate_topk,\n",
    "    plot_precision_recall_roc,\n",
    "    evaluate_random_recommender\n",
    ")"
   ],
   "id": "46f2751041bb55e3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Supponiamo che ratings contenga una colonna 'predicted_rating' già calcolata\n",
    "# In caso contrario, puoi usare un modello qualsiasi per generarla\n",
    "\n",
    "# ESEMPIO: Accuracy + Long Tail\n",
    "metrics = evaluate_rating_predictions(ratings['rating'], ratings['predicted_rating'])\n",
    "print(\"Valutazione globale:\", metrics)\n",
    "\n",
    "long_tail_errors = analyze_long_tail_effect(ratings, prediction_column='predicted_rating')\n",
    "print(\"Errore medio long tail vs head:\\n\", long_tail_errors)\n",
    "\n",
    "# ESEMPIO: Visualizzazione errori\n",
    "plot_avg_error_by_genre(ratings, movies)\n",
    "plot_avg_error_by_popularity(ratings)\n",
    "\n",
    "# ESEMPIO: Ranking Spearman per un utente\n",
    "user_id = ratings['userId'].sample(1).values[0]\n",
    "user_df = ratings[ratings['userId'] == user_id]\n",
    "correlation = evaluate_ranking_spearman(user_df['rating'], user_df['predicted_rating'])\n",
    "print(f\"Spearman correlation per l'utente {user_id}: {correlation:.3f}\")\n",
    "\n",
    "# ESEMPIO: Top-k evaluation (supponendo y_true e y_scores binari)\n",
    "# Qui facciamo un esempio fittizio\n",
    "true_items = set(user_df[user_df['rating'] >= 4]['movieId'])\n",
    "all_items = list(user_df['movieId'])\n",
    "scores = list(user_df['predicted_rating'])\n",
    "\n",
    "# Costruiamo y_true binario: 1 se è un item rilevante\n",
    "y_true = [1 if item in true_items else 0 for item in all_items]\n",
    "\n",
    "topk_result = evaluate_topk(y_true, scores, k=10)\n",
    "print(\"Top-K Evaluation:\", topk_result)\n",
    "\n",
    "# Curve Precision-Recall e ROC\n",
    "plot_precision_recall_roc(y_true, scores)\n",
    "\n",
    "# ESEMPIO: Benchmark con Random Recommender\n",
    "random_baseline = evaluate_random_recommender(true_items, all_items, k=10)\n",
    "print(\"Random Recommender Benchmark:\", random_baseline)"
   ],
   "id": "8aee1ed9ac1f137d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "models_outputs = {\n",
    "    'Global Naive Bayes': df_global_nb,\n",
    "    'User Naive Bayes': df_user_nb,\n",
    "    'Matrix Factorization': df_mf,\n",
    "    'Random': df_random\n",
    "}\n",
    "\n",
    "for name, df in models_outputs.items():\n",
    "    print(f\"\\n============================\")\n",
    "    print(f\"📊 Risultati per: {name}\")\n",
    "\n",
    "    # Valutazione classica\n",
    "    metrics = evaluate_rating_predictions(df['rating'].dropna(), df['predicted_rating'].dropna())\n",
    "    print(\"📌 Accuracy:\", metrics)\n",
    "\n",
    "    # Long Tail\n",
    "    long_tail = analyze_long_tail_effect(df.dropna(subset=['rating']))\n",
    "    print(\"🐍 Long Tail Error:\")\n",
    "    print(long_tail)\n",
    "\n",
    "    # Top-K Example su un utente qualsiasi (solo se abbiamo ground truth)\n",
    "    sample_user = df['userId'].iloc[0]\n",
    "    user_df = df[df['userId'] == sample_user].dropna(subset=['rating'])\n",
    "\n",
    "    if not user_df.empty:\n",
    "        # costruiamo y_true e scores\n",
    "        true_items = set(user_df[user_df['rating'] >= 4]['movieId'])\n",
    "        movie_ids = user_df['movieId'].tolist()\n",
    "        scores = user_df['predicted_rating'].tolist()\n",
    "        y_true = [1 if m in true_items else 0 for m in movie_ids]\n",
    "\n",
    "        topk = evaluate_topk(y_true, scores, k=10)\n",
    "        print(\"🎯 Top-K:\", topk)\n",
    "\n",
    "        # Correlazione ranking\n",
    "        spearman = evaluate_ranking_spearman(user_df['rating'], user_df['predicted_rating'])\n",
    "        print(f\"🔗 Spearman correlation: {spearman:.3f}\")\n",
    "\n",
    "        # Curve\n",
    "        plot_precision_recall_roc(y_true, scores)\n"
   ],
   "id": "d58172f97bf44907"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
