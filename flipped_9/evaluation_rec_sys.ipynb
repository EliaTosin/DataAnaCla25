{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-14T12:16:18.779852Z",
     "start_time": "2025-04-14T12:16:18.777140Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T12:16:18.832489Z",
     "start_time": "2025-04-14T12:16:18.830115Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "random.seed(42)"
   ],
   "id": "e8f99a01390f6270",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T12:16:18.897Z",
     "start_time": "2025-04-14T12:16:18.875524Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DATA_PATH = \"../ml-latest-small\"\n",
    "\n",
    "ratings = pd.read_csv(os.path.join(DATA_PATH, \"ratings.csv\"))\n",
    "movies = pd.read_csv(os.path.join(DATA_PATH, \"movies.csv\"))\n",
    "tags = pd.read_csv(os.path.join(DATA_PATH, \"tags.csv\"))"
   ],
   "id": "c6c31514452fb88f",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Unconstrained Matrix Factorisation based Collaborative Filtering (week 7)",
   "id": "54d40534a6a27bac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T12:16:34.715392Z",
     "start_time": "2025-04-14T12:16:18.928094Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_ecommerce= pd.read_json('../1_ecommerce.jsonl', lines=True)\n",
    "\n",
    "# for each session, we will create a list of items that the user has clicked on, removing duplicates\n",
    "clicks_items_list = []\n",
    "carts_items_list = []\n",
    "orders_items_list = []\n",
    "\n",
    "for events in df_ecommerce.events:\n",
    "    clicks = []\n",
    "    carts = []\n",
    "    orders = []\n",
    "    for e in events:\n",
    "        if e['type'] == 'clicks':\n",
    "            clicks.append(e['aid'])\n",
    "        if e['type'] == 'carts':\n",
    "            carts.append(e['aid'])\n",
    "        if e['type'] == 'orders':\n",
    "            orders.append(e['aid'])\n",
    "\n",
    "    clicks_items_list.append(list(clicks))\n",
    "    carts_items_list.append(list(carts))\n",
    "    orders_items_list.append(list(orders))\n",
    "\n",
    "df_ecommerce['clicks'] = clicks_items_list\n",
    "df_ecommerce['carts'] = carts_items_list\n",
    "df_ecommerce['orders'] = orders_items_list\n",
    "df = df_ecommerce.drop(columns=[\"events\"], axis=1)\n",
    "\n",
    "clicks_length = df_ecommerce['clicks'].apply(len)\n",
    "carts_length = df_ecommerce['carts'].apply(len)\n",
    "orders_length = df_ecommerce['orders'].apply(len)\n",
    "\n",
    "# takes only the session with items length higher than 20\n",
    "df_truncated = df[df['clicks'].apply(lambda x: len(x) > 10)]\n",
    "df = df_truncated.copy()\n",
    "# redefine index\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Explode each column (clicks, carts, orders)\n",
    "df_clicks = df[['session', 'clicks']].explode('clicks').rename(columns={'clicks': 'item'}).dropna(subset=['item'])\n",
    "df_carts = df[['session', 'carts']].explode('carts').rename(columns={'carts': 'item'}).dropna(subset=['item'])\n",
    "df_orders = df[['session', 'orders']].explode('orders').rename(columns={'orders': 'item'}).dropna(subset=['item'])\n",
    "\n",
    "# Concatenate the exploded dataframes\n",
    "df_concat = pd.concat([df_clicks, df_carts, df_orders])\n",
    "\n",
    "# Create a new column for each category indicating whether the item is present in that category\n",
    "df_concat['click'] = df_concat['item'].isin(df_clicks['item']).astype(int)\n",
    "df_concat['cart'] = df_concat['item'].isin(df_carts['item']).astype(int)\n",
    "df_concat['order'] = df_concat['item'].isin(df_orders['item']).astype(int)\n",
    "\n",
    "# Drop duplicates based on session and item\n",
    "df_concat = df_concat.drop_duplicates(subset=['session', 'item'])"
   ],
   "id": "ef990b1d18a15e78",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T12:16:34.752840Z",
     "start_time": "2025-04-14T12:16:34.747089Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def sgd_matrix_factorization(df, k=10, alpha=0.01, lambda_reg=0.1, num_epochs=20, w_click=1, w_cart=3, w_order=5, test_size=0.2, validation_size=0.1):\n",
    "    # Map session (users) and items to consecutive indices\n",
    "    users = {u: i for i, u in enumerate(df['session'].unique())}\n",
    "    items = {i: j for j, i in enumerate(df['item'].unique())}\n",
    "\n",
    "    num_users = len(users)\n",
    "    num_items = len(items)\n",
    "\n",
    "    # Initialize U, V, and biases\n",
    "    U = np.random.rand(num_users, k)\n",
    "    V = np.random.rand(num_items, k)\n",
    "    b_u = np.zeros(num_users)\n",
    "    b_i = np.zeros(num_items)\n",
    "    b = 0  # Global bias\n",
    "\n",
    "    # Create (u, i, r_ui) tuples for all interactions\n",
    "    data = []\n",
    "\n",
    "    # iter throw all the rows of the dataframe\n",
    "    # and create a list of tuples (user, item, rating)\n",
    "    # Assign ratings to interactions\n",
    "    for _, row in df.iterrows():\n",
    "        u = users[row['session']]\n",
    "        i = items[row['item']]\n",
    "\n",
    "        # if this user has perform an action on this item, we assign a rating based on the action\n",
    "        # type starting from the most important one\n",
    "        # (order > cart > click)\n",
    "        if row['order'] > 0:\n",
    "            r_ui = w_order\n",
    "        elif row['cart'] > 0:\n",
    "            r_ui = w_cart\n",
    "        elif row['click'] > 0:\n",
    "            r_ui = w_click\n",
    "        else:\n",
    "            continue  # Skip interactions with no recorded action\n",
    "\n",
    "        data.append((u, i, r_ui))\n",
    "\n",
    "    # Split the data into training, validation, and test sets\n",
    "    train_data, test_data = train_test_split(data, test_size=test_size, random_state=42)\n",
    "    train_data, val_data = train_test_split(train_data, test_size=validation_size, random_state=42)\n",
    "\n",
    "    # Compute global bias as the mean rating of the training set\n",
    "    if train_data:\n",
    "        b = np.mean([r for _, _, r in train_data])\n",
    "\n",
    "    print(\"Datasets created! Starting training...\")\n",
    "\n",
    "    # Training using SGD\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        np.random.shuffle(train_data)  # Shuffle data to improve convergence\n",
    "        total_loss = 0\n",
    "        total_val_loss = 0\n",
    "\n",
    "        U_temp = U.copy()\n",
    "        V_temp = V.copy()\n",
    "\n",
    "        # Training phase\n",
    "        for u, i, r_ui in train_data:\n",
    "            # Predict the rating using the current model parameters\n",
    "            # b is the global bias, b_u[u] is the user bias, b_i[i] is the item bias\n",
    "            pred = b + b_u[u] + b_i[i] + np.dot(U[u, :], V[i, :])\n",
    "\n",
    "            # Calculate the error\n",
    "            e_ui = r_ui - pred  # Error\n",
    "\n",
    "            # Update biases\n",
    "            # alpha is the learning rate, lambda_reg is the regularization parameter\n",
    "            b_u[u] += alpha * (e_ui - lambda_reg * b_u[u])\n",
    "            b_i[i] += alpha * (e_ui - lambda_reg * b_i[i])\n",
    "\n",
    "            # Update latent factors using temporary matrices\n",
    "            U_temp[u, :] += alpha * (e_ui * V[i, :] - lambda_reg * U[u, :])\n",
    "            V_temp[i, :] += alpha * (e_ui * U[u, :] - lambda_reg * V[i, :])\n",
    "\n",
    "            total_loss += e_ui ** 2  # Sum of squared errors\n",
    "        total_loss /= len(train_data)\n",
    "        total_loss = np.sqrt(total_loss)\n",
    "\n",
    "        # Copy back updated matrices\n",
    "        U = U_temp\n",
    "        V = V_temp\n",
    "\n",
    "        # Validation phase (calculate validation loss)\n",
    "        for u, i, r_ui in val_data:\n",
    "            pred = b + b_u[u] + b_i[i] + np.dot(U[u, :], V[i, :])\n",
    "            e_ui = r_ui - pred  # Error\n",
    "            total_val_loss += e_ui ** 2\n",
    "        total_val_loss /= len(val_data)\n",
    "        total_val_loss = np.sqrt(total_val_loss)\n",
    "\n",
    "        # Append losses for the current epoch\n",
    "        losses.append(total_loss)\n",
    "        val_losses.append(total_val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, RMSE Train Loss: {total_loss:.4f}, RMSE Val Loss: {total_val_loss:.4f}\")\n",
    "\n",
    "    # Test phase (calculate test loss)\n",
    "    test_loss = 0\n",
    "    for u, i, r_ui in test_data:\n",
    "        pred = b + b_u[u] + b_i[i] + np.dot(U[u, :], V[i, :])\n",
    "        e_ui = r_ui - pred  # Error\n",
    "        test_loss += e_ui ** 2\n",
    "    test_loss /= len(test_data)\n",
    "    test_loss = np.sqrt(test_loss)\n",
    "\n",
    "    print(f\"Final Test RMSE: {test_loss:.4f}\")\n",
    "\n",
    "    return U, V, b_u, b_i, b, losses, val_losses, test_data"
   ],
   "id": "bf5ae7de393eda7f",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T12:17:17.787853Z",
     "start_time": "2025-04-14T12:16:34.792572Z"
    }
   },
   "cell_type": "code",
   "source": "U, V, b_u, b_i, b, _, _, test_data = sgd_matrix_factorization(df_concat, k=20, alpha=0.001, num_epochs=1, w_click=1, w_order=2, w_cart=3)",
   "id": "e5611f564625e996",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets created! Starting training...\n",
      "Epoch 1/1, RMSE Train Loss: 5.0847, RMSE Val Loss: 4.3136\n",
      "Final Test RMSE: 4.3113\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Naive Bayes CF Model Based (Week6)\n",
    "\n",
    "Qui ho ridotto a 2 classi in modo da essere uguale alla week8 (content-based)\n",
    "\n",
    "```python\n",
    "user_data['label'] = user_data['rating'].apply(lambda r: 1 if r >= 4 else (0 if r <= 2 else None))\n",
    "```"
   ],
   "id": "7019d53115e25098"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T12:17:17.826834Z",
     "start_time": "2025-04-14T12:17:17.819352Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_ratings = ratings.copy()\n",
    "\n",
    "df_ratings.loc[df_ratings['rating'] <= 2, 'rating_ordinal'] = \"Don't like\"\n",
    "df_ratings.loc[df_ratings['rating'] >= 4, 'rating_ordinal'] = \"Like\"\n",
    "\n",
    "ratings_ordinals = df_ratings['rating_ordinal'].unique()\n",
    "\n",
    "all_films = df_ratings[\"movieId\"].unique()"
   ],
   "id": "ed48ad65a701138d",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T12:17:17.912004Z",
     "start_time": "2025-04-14T12:17:17.871380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Prendi solo le righe con rating_ordinal noto\n",
    "df_known = df_ratings.dropna(subset=['rating_ordinal'])\n",
    "\n",
    "# Fai lo split tra train e test\n",
    "# df_train, df_test = train_test_split(df_known, test_size=0.05, random_state=42) #3106 tuples\n",
    "df_train, df_test = train_test_split(df_known, test_size=0.005, random_state=42) #311 tuples ~1min\n",
    "\n",
    "# Ricrea la matrice user-movie SOLO con i dati del training set\n",
    "df_train_user_movie = df_train.pivot(index='userId', columns='movieId', values='rating_ordinal')\n",
    "\n",
    "# Il test sarà una lista di (userId, movieId, rating_ordinal) da prevedere\n",
    "df_test = df_test[['userId', 'movieId', 'rating_ordinal']]"
   ],
   "id": "8a1e9b04678ca338",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T12:17:17.949035Z",
     "start_time": "2025-04-14T12:17:17.944908Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test_naive_cf_model_based(df_test, df_train_user_movie):\n",
    "\n",
    "    df_predictions = df_test.copy()\n",
    "\n",
    "    for idx, row in enumerate(df_predictions.itertuples()):\n",
    "        userId = row.userId\n",
    "        movieId = row.movieId\n",
    "\n",
    "        # Film visti dall'utente, dai dati di TRAIN\n",
    "        try:\n",
    "            films_seen = df_train_user_movie.loc[userId].dropna().index\n",
    "        except KeyError:\n",
    "            # Utente non presente nel train (cold start) → skip\n",
    "            continue\n",
    "\n",
    "        category_probs = {}\n",
    "\n",
    "        for category in ratings_ordinals:\n",
    "            df_current_movie = df_train[(df_train['movieId'] == movieId)]\n",
    "            df_current_movie_category = df_current_movie[(df_current_movie['rating_ordinal'] == category)]\n",
    "\n",
    "            if len(df_current_movie) == 0:\n",
    "                continue  # impossibile stimare, nessuno ha votato il film\n",
    "\n",
    "            users_with_category = df_current_movie_category['userId'].unique()\n",
    "\n",
    "            p_cat = len(df_current_movie_category) / len(df_current_movie)\n",
    "            probs = [p_cat]\n",
    "\n",
    "            for film_seen in films_seen:\n",
    "                try:\n",
    "                    assigned_rating = df_train_user_movie.loc[userId, film_seen]\n",
    "                except KeyError:\n",
    "                    continue  # Film non presente per quell’utente\n",
    "\n",
    "                df_seen_movie = df_train[df_train['movieId'] == film_seen]\n",
    "                df_seen_movie_filtered = df_seen_movie[df_seen_movie['userId'].isin(users_with_category)]\n",
    "                df_seen_movie_same_rating = df_seen_movie_filtered[df_seen_movie_filtered['rating_ordinal'] == assigned_rating]\n",
    "\n",
    "                if len(df_seen_movie_filtered) == 0:\n",
    "                    continue\n",
    "\n",
    "                p_cond = len(df_seen_movie_same_rating) / len(df_seen_movie_filtered)\n",
    "                probs.append(p_cond)\n",
    "\n",
    "            category_probs[category] = np.prod(probs)\n",
    "\n",
    "        if category_probs:\n",
    "            predicted = max(category_probs.items(), key=lambda x: x[1])[0]\n",
    "            df_predictions.loc[(df_predictions['userId'] == userId) & (df_predictions['movieId'] == movieId), 'predicted'] = predicted\n",
    "\n",
    "        if idx % 50 == 0:\n",
    "            print(f\"Processed {idx}/{len(df_predictions)}\")\n",
    "\n",
    "    return df_predictions"
   ],
   "id": "6199cf520705d832",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T12:18:34.593702Z",
     "start_time": "2025-04-14T12:17:17.987909Z"
    }
   },
   "cell_type": "code",
   "source": "df_predictions = test_naive_cf_model_based(df_test, df_train_user_movie)",
   "id": "3d533add08f443b7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0/311\n",
      "Processed 50/311\n",
      "Processed 100/311\n",
      "Processed 150/311\n",
      "Processed 200/311\n",
      "Processed 250/311\n",
      "Processed 300/311\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T12:18:34.658221Z",
     "start_time": "2025-04-14T12:18:34.654072Z"
    }
   },
   "cell_type": "code",
   "source": "df_predictions.head(10)",
   "id": "9b8e1de1c505154b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       userId  movieId rating_ordinal predicted\n",
       "66214     426     2797           Like      Like\n",
       "73518     474     1394           Like      Like\n",
       "15301     100       11           Like      Like\n",
       "23465     160     2078           Like      Like\n",
       "77851     483    79242           Like      Like\n",
       "22670     155     2959           Like      Like\n",
       "75461     477      374     Don't like      Like\n",
       "22474     153     2571     Don't like      Like\n",
       "90090     586     1784           Like      Like\n",
       "51118     330     2739           Like      Like"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating_ordinal</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66214</th>\n",
       "      <td>426</td>\n",
       "      <td>2797</td>\n",
       "      <td>Like</td>\n",
       "      <td>Like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73518</th>\n",
       "      <td>474</td>\n",
       "      <td>1394</td>\n",
       "      <td>Like</td>\n",
       "      <td>Like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15301</th>\n",
       "      <td>100</td>\n",
       "      <td>11</td>\n",
       "      <td>Like</td>\n",
       "      <td>Like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23465</th>\n",
       "      <td>160</td>\n",
       "      <td>2078</td>\n",
       "      <td>Like</td>\n",
       "      <td>Like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77851</th>\n",
       "      <td>483</td>\n",
       "      <td>79242</td>\n",
       "      <td>Like</td>\n",
       "      <td>Like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22670</th>\n",
       "      <td>155</td>\n",
       "      <td>2959</td>\n",
       "      <td>Like</td>\n",
       "      <td>Like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75461</th>\n",
       "      <td>477</td>\n",
       "      <td>374</td>\n",
       "      <td>Don't like</td>\n",
       "      <td>Like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22474</th>\n",
       "      <td>153</td>\n",
       "      <td>2571</td>\n",
       "      <td>Don't like</td>\n",
       "      <td>Like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90090</th>\n",
       "      <td>586</td>\n",
       "      <td>1784</td>\n",
       "      <td>Like</td>\n",
       "      <td>Like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51118</th>\n",
       "      <td>330</td>\n",
       "      <td>2739</td>\n",
       "      <td>Like</td>\n",
       "      <td>Like</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T12:18:34.721971Z",
     "start_time": "2025-04-14T12:18:34.719179Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Skipping the NaN values (where we don't have that user in train / film without ratings)\n",
    "df_predictions.dropna(subset=['predicted'], inplace=True)"
   ],
   "id": "fcbccf8041c43594",
   "outputs": [],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T12:18:34.770387Z",
     "start_time": "2025-04-14T12:18:34.762848Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_predictions['rating_label'] = df_predictions['rating_ordinal'].map({\n",
    "    \"Don't like\": 0,\n",
    "    \"Like\": 1\n",
    "})\n",
    "\n",
    "df_predictions['pred_label'] = df_predictions['predicted'].map({\n",
    "    \"Don't like\": 0,\n",
    "    \"Like\": 1\n",
    "})\n",
    "\n",
    "print(classification_report(df_predictions['rating_label'], df_predictions['pred_label']))"
   ],
   "id": "57c45eb66ec2c752",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.08      0.14        60\n",
      "           1       0.81      0.98      0.89       238\n",
      "\n",
      "    accuracy                           0.80       298\n",
      "   macro avg       0.68      0.53      0.52       298\n",
      "weighted avg       0.76      0.80      0.74       298\n",
      "\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Bayes Classification Content-based (week 8)",
   "id": "f5debd8f94280067"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T12:18:34.838471Z",
     "start_time": "2025-04-14T12:18:34.821597Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## 🧹 Preprocess Movie Metadata\n",
    "tags_agg = tags.copy().groupby(\"movieId\")[\"tag\"].apply(lambda x: \" \".join(x)).reset_index()\n",
    "movies_df = movies.copy().merge(tags_agg, on=\"movieId\", how=\"left\")\n",
    "movies_df[\"tag\"] = movies_df[\"tag\"].fillna(\"\")"
   ],
   "id": "2113ce104faf20a9",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 1. User-Specific Naive Bayes Recommender",
   "id": "4e800e705b0a614e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T12:18:34.894611Z",
     "start_time": "2025-04-14T12:18:34.871615Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Clean the title by removing the year in parentheses\n",
    "def clean_title(title):\n",
    "    return re.sub(r'\\s*\\(\\d{4}\\)', '', title)\n",
    "\n",
    "metadata = movies_df[[\"movieId\", \"title\", \"genres\"]]\n",
    "metadata.loc[:, 'title'] = metadata['title'].apply(clean_title)\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "genres_encoded = mlb.fit_transform(metadata['genres'].str.split('|'))\n",
    "# Create a DataFrame with the encoded genres\n",
    "genres_df = pd.DataFrame(genres_encoded, columns=mlb.classes_)\n",
    "# Concatenate the original metadata with the encoded genres\n",
    "metadata = pd.concat([metadata[['movieId', 'title']], genres_df], axis=1)\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "\n",
    "# Create a preprocessor that transforms the movie metadata:\n",
    "# - Applies TF-IDF vectorization to the cleaned 'title' column to extract textual features.\n",
    "# - Passes through the binary genre columns (already transformed by MultiLabelBinarizer).\n",
    "# - Drops any remaining columns that are not explicitly selected.\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('tfidf', tfidf, 'title'),\n",
    "        ('genres', 'passthrough', genres_df.columns)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")"
   ],
   "id": "3c899a8aa448d3a3",
   "outputs": [],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T12:18:34.957557Z",
     "start_time": "2025-04-14T12:18:34.954581Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def predict_single_movie(user_id, movie_id, ratings_train):\n",
    "    ratings = ratings_train.copy()\n",
    "\n",
    "    # Step 1: Prepare user data\n",
    "    user_ratings = ratings[ratings['userId'] == user_id]\n",
    "    user_data = pd.merge(user_ratings, metadata, on='movieId')\n",
    "\n",
    "    # Step 2: Create binary labels\n",
    "    user_data['label'] = user_data['rating'].apply(lambda r: 1 if r >= 4 else (0 if r <= 2 else None))\n",
    "    user_data = user_data.dropna(subset=['label'])\n",
    "    user_data['label'] = user_data['label'].astype(int)\n",
    "\n",
    "    if user_data.empty:\n",
    "        print(\"User has insufficient data.\")\n",
    "        return None\n",
    "\n",
    "    # Step 3: Train the model\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', MultinomialNB())\n",
    "    ])\n",
    "\n",
    "    X_train = user_data.drop(columns=['userId', 'rating', 'label', 'timestamp', 'rating_ordinal'])\n",
    "    y_train = user_data['label']\n",
    "\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Step 4: Check if the movie has already been watched\n",
    "    if movie_id in user_ratings['movieId'].values:\n",
    "        print(\"The movie has already been watched by the user.\")\n",
    "        return None\n",
    "\n",
    "    # Step 5: Extract features of the requested movie\n",
    "    movie_row = metadata[metadata['movieId'] == movie_id]\n",
    "\n",
    "    if movie_row.empty:\n",
    "        print(\"Movie ID not found in the metadata.\")\n",
    "        return None\n",
    "\n",
    "    input_cols = list(X_train.columns)\n",
    "    movie_features = movie_row[input_cols]\n",
    "\n",
    "    # Step 6: Predict the probability of liking the movie\n",
    "    probs = pipeline.predict_proba(movie_features)[0]  # P(liked | features)\n",
    "    if len(probs) < 2:\n",
    "        print(f\"The model for user {user_id} has seen only one class (not like/like).\")\n",
    "        return None\n",
    "    else:\n",
    "        prob = probs[1]\n",
    "\n",
    "    return \"Like\" if prob >= 0.5 else \"Don't like\""
   ],
   "id": "621d69e8f2d31476",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T12:18:35.014234Z",
     "start_time": "2025-04-14T12:18:35.003808Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chosen_user = random.choice(ratings['userId'].unique())\n",
    "chosen_film = random.choice(metadata['movieId'].unique())\n",
    "\n",
    "predict_single_movie(user_id=chosen_user, movie_id=chosen_film, ratings_train=df_train)"
   ],
   "id": "ad8b0d9b5380540c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Like'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 2. Global Content-Based Recommender (Single Model for All Users)",
   "id": "14fffcebd321f853"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T12:18:35.052004Z",
     "start_time": "2025-04-14T12:18:35.047728Z"
    }
   },
   "cell_type": "code",
   "source": [
    "metadata_with_tags = metadata.merge(movies_df[[\"movieId\", \"tag\"]], on='movieId', how='left')\n",
    "\n",
    "preprocessor_global = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('tfidf', tfidf, 'title'),\n",
    "        ('genres', 'passthrough', genres_df.columns),\n",
    "        ('tfidf_tag', tfidf, 'tag')\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")"
   ],
   "id": "1d8958d2b6dddc1",
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T12:18:35.376380Z",
     "start_time": "2025-04-14T12:18:35.094344Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_global_model(ratings_train):\n",
    "    ratings = ratings_train.copy()\n",
    "    # Step 1: Prepare data\n",
    "    data = pd.merge(ratings, metadata_with_tags, on='movieId')\n",
    "\n",
    "    # Step 2: Create binary labels\n",
    "    data['label'] = data['rating'].apply(lambda r: 1 if r >= 4 else (0 if r <= 2 else None))\n",
    "    data = data.dropna(subset=['label'])\n",
    "    data['label'] = data['label'].astype(int)\n",
    "\n",
    "    # Step 3: Train the model\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor_global),\n",
    "        ('classifier', MultinomialNB())\n",
    "    ])\n",
    "\n",
    "    X = data.drop(columns=['userId', 'movieId', 'rating', 'label', 'timestamp', 'rating_ordinal'])\n",
    "    y = data['label']\n",
    "\n",
    "    model = pipeline.fit(X, y)\n",
    "    return model, X.columns\n",
    "\n",
    "model_global, train_columns = train_global_model(df_train)\n",
    "\n",
    "def recommend_global(user_id, movie_id, ratings_train):\n",
    "    ratings = ratings_train.copy()\n",
    "\n",
    "    # Step 4: Check if the movie has already been watched\n",
    "    user_ratings = ratings[ratings['userId'] == user_id]\n",
    "    if movie_id in user_ratings['movieId'].values:\n",
    "        print(\"The movie has already been watched by the user.\")\n",
    "        return None\n",
    "\n",
    "    # Step 5: Extract features of the requested movie\n",
    "    movie_row = metadata_with_tags[metadata_with_tags['movieId'] == movie_id]\n",
    "\n",
    "    if movie_row.empty:\n",
    "        print(\"Movie ID not found in the metadata.\")\n",
    "        return None\n",
    "    movie_features = movie_row[train_columns]\n",
    "\n",
    "    # Step 6: Predict the probability of liking the movie\n",
    "    probs = model_global.predict_proba(movie_features)[0]  # P(liked | features)\n",
    "    if len(probs) < 2:\n",
    "        print(f\"The model for user {user_id} has seen only one class (not like/like).\")\n",
    "        return None\n",
    "    else:\n",
    "        prob = probs[1]\n",
    "\n",
    "    return \"Like\" if prob >= 0.5 else \"Don't like\""
   ],
   "id": "90a58090827be11a",
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T12:18:35.411466Z",
     "start_time": "2025-04-14T12:18:35.406051Z"
    }
   },
   "cell_type": "code",
   "source": "recommend_global(user_id=chosen_user, movie_id=chosen_film, ratings_train=df_train)",
   "id": "207a8ae7c2df056d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Like'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T12:21:06.600733Z",
     "start_time": "2025-04-14T12:21:06.597884Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test_naive_cf_content_based(df_param, df_train):\n",
    "    df_tmp = df_param.copy()\n",
    "\n",
    "    preds_user = []\n",
    "    preds_global = []\n",
    "    for idx, row in enumerate(df_tmp.itertuples()):\n",
    "        userId = row.userId\n",
    "        movieId = row.movieId\n",
    "\n",
    "        pred_user = predict_single_movie(userId, movieId, ratings_train=df_train)\n",
    "        pred_global = recommend_global(userId, movieId, ratings_train=df_train)\n",
    "\n",
    "        preds_user.append(pred_user)\n",
    "        preds_global.append(pred_global)\n",
    "        # df_tmp.loc[(df_predictions['userId'] == userId) & (df_predictions['movieId'] == movieId), 'pred_user'] = pred_user\n",
    "        # df_tmp.loc[(df_predictions['userId'] == userId) & (df_predictions['movieId'] == movieId), 'pred_glob'] = pred_global\n",
    "\n",
    "    return preds_user, preds_global"
   ],
   "id": "f3dc49ffcc8894ad",
   "outputs": [],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T12:21:08.816519Z",
     "start_time": "2025-04-14T12:21:08.811725Z"
    }
   },
   "cell_type": "code",
   "source": "df_predictions.head()",
   "id": "56788dd38d48fd49",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       userId  movieId rating_ordinal predicted  rating_label  pred_label\n",
       "66214     426     2797           Like      Like             1           1\n",
       "73518     474     1394           Like      Like             1           1\n",
       "15301     100       11           Like      Like             1           1\n",
       "23465     160     2078           Like      Like             1           1\n",
       "77851     483    79242           Like      Like             1           1"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating_ordinal</th>\n",
       "      <th>predicted</th>\n",
       "      <th>rating_label</th>\n",
       "      <th>pred_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66214</th>\n",
       "      <td>426</td>\n",
       "      <td>2797</td>\n",
       "      <td>Like</td>\n",
       "      <td>Like</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73518</th>\n",
       "      <td>474</td>\n",
       "      <td>1394</td>\n",
       "      <td>Like</td>\n",
       "      <td>Like</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15301</th>\n",
       "      <td>100</td>\n",
       "      <td>11</td>\n",
       "      <td>Like</td>\n",
       "      <td>Like</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23465</th>\n",
       "      <td>160</td>\n",
       "      <td>2078</td>\n",
       "      <td>Like</td>\n",
       "      <td>Like</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77851</th>\n",
       "      <td>483</td>\n",
       "      <td>79242</td>\n",
       "      <td>Like</td>\n",
       "      <td>Like</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T12:21:14.238811Z",
     "start_time": "2025-04-14T12:21:11.311653Z"
    }
   },
   "cell_type": "code",
   "source": "u, g = test_naive_cf_content_based(df_predictions, df_train=df_train)",
   "id": "49121b96c6ace512",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model for user 586 has seen only one class (not like/like).\n",
      "The model for user 519 has seen only one class (not like/like).\n",
      "The model for user 74 has seen only one class (not like/like).\n",
      "The model for user 74 has seen only one class (not like/like).\n",
      "The model for user 72 has seen only one class (not like/like).\n",
      "The model for user 60 has seen only one class (not like/like).\n",
      "The model for user 319 has seen only one class (not like/like).\n",
      "The model for user 523 has seen only one class (not like/like).\n",
      "The model for user 169 has seen only one class (not like/like).\n",
      "The model for user 533 has seen only one class (not like/like).\n",
      "The model for user 348 has seen only one class (not like/like).\n",
      "The model for user 43 has seen only one class (not like/like).\n",
      "The model for user 49 has seen only one class (not like/like).\n",
      "The model for user 171 has seen only one class (not like/like).\n",
      "The model for user 142 has seen only one class (not like/like).\n",
      "The model for user 106 has seen only one class (not like/like).\n",
      "The model for user 43 has seen only one class (not like/like).\n",
      "The model for user 586 has seen only one class (not like/like).\n",
      "The model for user 56 has seen only one class (not like/like).\n",
      "The model for user 585 has seen only one class (not like/like).\n",
      "The model for user 80 has seen only one class (not like/like).\n",
      "The model for user 12 has seen only one class (not like/like).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  None,\n",
       "  'Like',\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  None,\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  None,\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  None,\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  'Like',\n",
       "  None,\n",
       "  'Like',\n",
       "  None,\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  None,\n",
       "  None,\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  'Like',\n",
       "  None,\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  None,\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  \"Don't like\",\n",
       "  \"Don't like\",\n",
       "  None,\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  None,\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  None,\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  None,\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  \"Don't like\",\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  'Like',\n",
       "  None,\n",
       "  \"Don't like\",\n",
       "  None,\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  None,\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  None,\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  None,\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  None,\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  None,\n",
       "  \"Don't like\",\n",
       "  None,\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like'],\n",
       " ['Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  \"Don't like\",\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  \"Don't like\",\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  \"Don't like\",\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like',\n",
       "  'Like'])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T12:22:27.237862Z",
     "start_time": "2025-04-14T12:22:27.234808Z"
    }
   },
   "cell_type": "code",
   "source": "len(u), len(df_predictions)",
   "id": "44d97d24dd2c7b58",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(298, 298)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 79
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Evaluation",
   "id": "69f21698b1b4497a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T12:18:38.647761Z",
     "start_time": "2025-04-14T12:18:38.646058Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Importa le funzioni di valutazione dal modulo esterno\n",
    "from eval_functions import (\n",
    "    evaluate_rating_predictions,\n",
    "    analyze_long_tail_effect,\n",
    "    plot_avg_error_by_genre,\n",
    "    plot_avg_error_by_popularity,\n",
    "    evaluate_ranking_spearman,\n",
    "    evaluate_topk,\n",
    "    plot_precision_recall_roc,\n",
    "    evaluate_random_recommender\n",
    ")"
   ],
   "id": "46f2751041bb55e3",
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T12:18:38.712400Z",
     "start_time": "2025-04-14T12:18:38.683193Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Supponiamo che ratings contenga una colonna 'predicted_rating' già calcolata\n",
    "# In caso contrario, puoi usare un modello qualsiasi per generarla\n",
    "\n",
    "# ESEMPIO: Accuracy + Long Tail\n",
    "metrics = evaluate_rating_predictions(ratings['rating'], ratings['predicted_rating'])\n",
    "print(\"Valutazione globale:\", metrics)\n",
    "\n",
    "long_tail_errors = analyze_long_tail_effect(ratings, prediction_column='predicted_rating')\n",
    "print(\"Errore medio long tail vs head:\\n\", long_tail_errors)\n",
    "\n",
    "# ESEMPIO: Visualizzazione errori\n",
    "plot_avg_error_by_genre(ratings, movies)\n",
    "plot_avg_error_by_popularity(ratings)\n",
    "\n",
    "# ESEMPIO: Ranking Spearman per un utente\n",
    "user_id = ratings['userId'].sample(1).values[0]\n",
    "user_df = ratings[ratings['userId'] == user_id]\n",
    "correlation = evaluate_ranking_spearman(user_df['rating'], user_df['predicted_rating'])\n",
    "print(f\"Spearman correlation per l'utente {user_id}: {correlation:.3f}\")\n",
    "\n",
    "# ESEMPIO: Top-k evaluation (supponendo y_true e y_scores binari)\n",
    "# Qui facciamo un esempio fittizio\n",
    "true_items = set(user_df[user_df['rating'] >= 4]['movieId'])\n",
    "all_items = list(user_df['movieId'])\n",
    "scores = list(user_df['predicted_rating'])\n",
    "\n",
    "# Costruiamo y_true binario: 1 se è un item rilevante\n",
    "y_true = [1 if item in true_items else 0 for item in all_items]\n",
    "\n",
    "topk_result = evaluate_topk(y_true, scores, k=10)\n",
    "print(\"Top-K Evaluation:\", topk_result)\n",
    "\n",
    "# Curve Precision-Recall e ROC\n",
    "plot_precision_recall_roc(y_true, scores)\n",
    "\n",
    "# ESEMPIO: Benchmark con Random Recommender\n",
    "random_baseline = evaluate_random_recommender(true_items, all_items, k=10)\n",
    "print(\"Random Recommender Benchmark:\", random_baseline)"
   ],
   "id": "8aee1ed9ac1f137d",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'predicted_rating'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:3805\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3804\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 3805\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcasted_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3806\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[0;32mindex.pyx:167\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mindex.pyx:196\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: 'predicted_rating'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[73], line 5\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Supponiamo che ratings contenga una colonna 'predicted_rating' già calcolata\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# In caso contrario, puoi usare un modello qualsiasi per generarla\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# ESEMPIO: Accuracy + Long Tail\u001B[39;00m\n\u001B[0;32m----> 5\u001B[0m metrics \u001B[38;5;241m=\u001B[39m evaluate_rating_predictions(ratings[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrating\u001B[39m\u001B[38;5;124m'\u001B[39m], \u001B[43mratings\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mpredicted_rating\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m)\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mValutazione globale:\u001B[39m\u001B[38;5;124m\"\u001B[39m, metrics)\n\u001B[1;32m      8\u001B[0m long_tail_errors \u001B[38;5;241m=\u001B[39m analyze_long_tail_effect(ratings, prediction_column\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpredicted_rating\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:4102\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   4100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mnlevels \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m   4101\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_multilevel(key)\n\u001B[0;32m-> 4102\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   4103\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[1;32m   4104\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m [indexer]\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3807\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(casted_key, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[1;32m   3808\u001B[0m         \u001B[38;5;28misinstance\u001B[39m(casted_key, abc\u001B[38;5;241m.\u001B[39mIterable)\n\u001B[1;32m   3809\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28many\u001B[39m(\u001B[38;5;28misinstance\u001B[39m(x, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m casted_key)\n\u001B[1;32m   3810\u001B[0m     ):\n\u001B[1;32m   3811\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m InvalidIndexError(key)\n\u001B[0;32m-> 3812\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[1;32m   3813\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m   3814\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[1;32m   3815\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[1;32m   3816\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[1;32m   3817\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n",
      "\u001B[0;31mKeyError\u001B[0m: 'predicted_rating'"
     ]
    }
   ],
   "execution_count": 73
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "models_outputs = {\n",
    "    'Global Naive Bayes': df_global_nb,\n",
    "    'User Naive Bayes': df_user_nb,\n",
    "    'Matrix Factorization': df_mf,\n",
    "    'Random': df_random\n",
    "}\n",
    "\n",
    "for name, df in models_outputs.items():\n",
    "    print(f\"\\n============================\")\n",
    "    print(f\"📊 Risultati per: {name}\")\n",
    "\n",
    "    # Valutazione classica\n",
    "    metrics = evaluate_rating_predictions(df['rating'].dropna(), df['predicted_rating'].dropna())\n",
    "    print(\"📌 Accuracy:\", metrics)\n",
    "\n",
    "    # Long Tail\n",
    "    long_tail = analyze_long_tail_effect(df.dropna(subset=['rating']))\n",
    "    print(\"🐍 Long Tail Error:\")\n",
    "    print(long_tail)\n",
    "\n",
    "    # Top-K Example su un utente qualsiasi (solo se abbiamo ground truth)\n",
    "    sample_user = df['userId'].iloc[0]\n",
    "    user_df = df[df['userId'] == sample_user].dropna(subset=['rating'])\n",
    "\n",
    "    if not user_df.empty:\n",
    "        # costruiamo y_true e scores\n",
    "        true_items = set(user_df[user_df['rating'] >= 4]['movieId'])\n",
    "        movie_ids = user_df['movieId'].tolist()\n",
    "        scores = user_df['predicted_rating'].tolist()\n",
    "        y_true = [1 if m in true_items else 0 for m in movie_ids]\n",
    "\n",
    "        topk = evaluate_topk(y_true, scores, k=10)\n",
    "        print(\"🎯 Top-K:\", topk)\n",
    "\n",
    "        # Correlazione ranking\n",
    "        spearman = evaluate_ranking_spearman(user_df['rating'], user_df['predicted_rating'])\n",
    "        print(f\"🔗 Spearman correlation: {spearman:.3f}\")\n",
    "\n",
    "        # Curve\n",
    "        plot_precision_recall_roc(y_true, scores)\n"
   ],
   "id": "d58172f97bf44907",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
